
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>9. Machine learning: Introduction &#8212; SCMA248 Introduction to Data Science</title>
    
  <link href="_static/css/theme.css" rel="stylesheet">
  <link href="_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="10. Unsupervised Machine Learning: K-means Clustering" href="Chapter8_Unsupervised_Learning.html" />
    <link rel="prev" title="8. Regression Analysis" href="Chapter6_Regression.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">SCMA248 Introduction to Data Science</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="intro.html">
   Welcome to SCMA248 Introduction to Data Science
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="Chapter1_Introduction_to_Data_Science.html">
   1. Introduction to Data Science
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Chapter2_Python_Basics.html">
   2. Python Basics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Chapter3_Data_Preparation.html">
   3. Data Selection
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Chapter4_Data_Visualization.html">
   6. Data Visualization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Chapter5_Practical_Statistics.html">
   7. Practical Statistics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Chapter6_Regression.html">
   8. Regression Analysis
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   9. Machine learning: Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Chapter8_Unsupervised_Learning.html">
   10. Unsupervised Machine Learning: K-means Clustering
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/Chapter7_Introduction_Machine_Learning.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/executablebooks/jupyter-book"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2FChapter7_Introduction_Machine_Learning.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/executablebooks/jupyter-book/master?urlpath=tree/docs/Chapter7_Introduction_Machine_Learning.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#what-is-machine-learning">
   9.1. What Is Machine Learning?
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#building-models-of-data">
     9.1.1. Building models of data
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#applications-of-machine-learning">
   9.2. Applications of Machine learning
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#categories-of-machine-learning">
   9.3. Categories of Machine Learning
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#supervised-learning">
     9.3.1. Supervised learning
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#classification-problems-more-examples-below">
       9.3.1.1. Classification problems (more examples below)
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#regression-problem-discussed-in-our-last-chatpter">
       9.3.1.2. Regression problem (discussed in our last chatpter)
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#unsupervised-learning-more-details-in-the-next-chapter">
     9.3.2. Unsupervised learning (more details in the next chapter)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#supervised-vs-unsupervised-learning-image-from-researchgate">
     9.3.3. Supervised vs Unsupervised Learning: image from researchgate
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#various-classification-regression-and-clustering-algorithms-image-from-scikit-learn">
     9.3.4. Various classification, regression and clustering algorithms: image from scikit-learn
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#machine-learning-basics-with-the-k-nearest-neighbors-algorithm">
   9.4. Machine Learning Basics with the K-Nearest Neighbors Algorithm
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#k-nearest-neighbours-can-be-summarized-as-follows">
     9.4.1. K-nearest neighbours can be summarized as follows:
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#knn-algorithm-s-theory">
     9.4.2. KNN algorithm’s theory
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#example-on-knn-classifiers-image-from-researchgate">
     9.4.3. Example on KNN classifiers: image from Researchgate
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#dataset-https-raw-githubusercontent-com-susanli2016-machine-learning-with-python-master-fruit-data-with-colors-txt">
     9.4.4. Dataset: https://raw.githubusercontent.com/susanli2016/Machine-Learning-with-Python/master/fruit_data_with_colors.txt
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#our-goals">
       9.4.4.1. Our goals
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#exploratory-data-analysis">
       9.4.4.2. Exploratory data analysis
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#what-we-observe-from-the-figures">
       9.4.4.3. What we observe from the figures?
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#preprocessing-train-test-split">
       9.4.4.4. Preprocessing: Train Test Split.
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#training-and-predictions">
       9.4.4.5. Training and Predictions
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#train-the-classifier-fit-the-estimator-using-the-training-data">
       9.4.4.6. Train the classifier (fit the estimator) using the training data¶
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#estimate-the-accuracy-of-the-classifier-on-future-data-using-the-test-data">
       9.4.4.7. Estimate the accuracy of the classifier on future data, using the test data
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#use-the-trained-knn-classifier-model-to-classify-new-previously-unseen-objects">
       9.4.4.8. Use the trained KNN classifier model to classify new, previously unseen objects
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#calculating-the-agorithm-accuracy">
       9.4.4.9. Calculating the agorithm accuracy
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#comparison-of-the-observed-and-predicted-values-from-both-training-and-test-data-sets">
       9.4.4.10. Comparison of the observed and predicted values from both training and test data sets
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#visualize-the-decision-regions-of-a-classifier">
       9.4.4.11. Visualize the decision regions of a classifier
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#decision-regions-for-two-features-height-and-mass">
       9.4.4.12. Decision regions for two features, height and mass.
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#decision-regions-for-two-features-height-and-width">
       9.4.4.13. Decision regions for two features, height and width.
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#visualization-of-dicision-boundary-with-knn-classification-for-a-dataset-with-only-two-features">
       9.4.4.14. Visualization of Dicision Boundary with KNN classification for a dataset with only two features.
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#width-mass-visualization">
       9.4.4.15. Width-Mass visualization
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#width-height-visualization">
       9.4.4.16. Width-height visualization
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#visualize-from-scratch-the-decision-regions-of-a-classifier">
       9.4.4.17. Visualize (from scratch) the decision regions of a classifier
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#model-evaluation-metrics-in-machine-learning">
   9.5. Model Evaluation Metrics in Machine Learning
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#how-can-we-figure-out-which-algorithm-is-the-most-effective">
     9.5.1. How can we figure out which algorithm is the most effective?
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#regression-related-metrics">
       9.5.1.1. Regression Related Metrics
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#classification-metrics">
       9.5.1.2. Classification Metrics
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#knn-model-with-color-score-and-other-feature-s">
       9.5.1.3. KNN model with
       <strong>
        color_score
       </strong>
       and other feature(s)
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#overfitting-and-underfitting-the-misclassification-rate-vs-k">
       9.5.1.4. Overfitting and Underfitting (The Misclassification Rate vs K)
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#overfitting-and-underfitting">
         9.5.1.4.1. Overfitting and Underfitting
        </a>
       </li>
      </ul>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#visualization-of-decision-regions-knn-model-with-two-features-height-and-color-score">
       9.5.1.5. Visualization of decision regions (KNN model with two features,
       <strong>
        height
       </strong>
       and
       <strong>
        color_score
       </strong>
       )
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#scaling-features-in-knn">
   9.6. Scaling Features in KNN
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#common-techniques-of-feature-scaling">
     9.6.1. Common techniques of feature scaling
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#feature-scaling-the-fruit-dataset">
     9.6.2. Feature Scaling the Fruit Dataset
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#model-validation">
   9.7. Model validation
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#holdout-sets">
     9.7.1. Holdout sets
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#cross-validation">
     9.7.2. Cross-validation
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#computing-cross-validated-metrics">
       9.7.2.1. Computing cross-validated metrics¶
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#hypertuning-model-parameters-using-grid-search">
     9.7.3. Hypertuning Model Parameters using Grid Search
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#using-the-best-parameter-to-make-prediction-and-result-evaluation">
       9.7.3.1. Using the best parameter to make prediction and result evaluation
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#swarmplots">
       9.7.3.2. Swarmplots
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#feature-scaling-the-fruit-dataset-width-and-height">
       9.7.3.3. Feature Scaling the Fruit Dataset (width and height)
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="machine-learning-introduction">
<h1><span class="section-number">9. </span>Machine learning: Introduction<a class="headerlink" href="#machine-learning-introduction" title="Permalink to this headline">¶</a></h1>
<p><strong>Machine learning</strong> is clearly one of the most powerful and significant technologies in the world today. And more importantly, we have yet to fully realize its potential. It will undoubtedly continue to make headlines for the foreseeable future.</p>
<p>Machine learning is <strong>a technique for transforming data into knowledge</strong>. In the last 50 years, there has been a data explosion. This vast amount of data is worthless until we analyze it and uncover the underlying patterns.</p>
<p>Machine learning techniques are being used to <strong>discover useful underlying patterns in complex data</strong> that would otherwise be difficult to find. Hidden patterns and problem knowledge can be used to predict future events and make a variety of complex decisions.</p>
<section id="what-is-machine-learning">
<h2><span class="section-number">9.1. </span>What Is Machine Learning?<a class="headerlink" href="#what-is-machine-learning" title="Permalink to this headline">¶</a></h2>
<p>Machine learning is the study of computer algorithms that can learn and develop on their own with experience and data.</p>
<p>It is considered to be a component of <strong>artificial intelligence</strong>.</p>
<p>Machine learning algorithms create a model based on <strong>training data</strong> to make predictions or decisions without having to be explicitly programmed to do so.</p>
<section id="building-models-of-data">
<h3><span class="section-number">9.1.1. </span>Building models of data<a class="headerlink" href="#building-models-of-data" title="Permalink to this headline">¶</a></h3>
<p>It makes more sense to think of machine learning as a means of <strong>building models of data</strong>.</p>
<p>Machine learning is fundamentally about building mathematical models that facilitate the understanding of data.</p>
<p>If we provide these models with <strong>tunable parameters</strong> that can be adapted to the observed data, we can call the program <strong>“learning” from the data</strong>.</p>
<p>These models can be used to <strong>predict and understand features of newly observed data</strong> after fitting them to previously seen data.</p>
<p><strong>Categories of Machine Learning</strong>: image from ceralytics
<img alt="Categories of Machine Learning: from ceralytics" src="https://www.ceralytics.com/wp-content/uploads/2019/08/machine-learning.jpg" /></p>
</section>
</section>
<section id="applications-of-machine-learning">
<h2><span class="section-number">9.2. </span>Applications of Machine learning<a class="headerlink" href="#applications-of-machine-learning" title="Permalink to this headline">¶</a></h2>
<p>Machine learning tasks can be used for a variety of things. Here are some examples of traditional machine learning tasks:</p>
<ol class="simple">
<li><p><strong>Recommendation systems</strong></p></li>
</ol>
<p><img alt="Netflix recommendation system: from medium" src="https://miro.medium.com/max/1400/1*QKQA8ylu1lCtOkJaa_gGaw.png" /></p>
<p>We come across a variety of online recommendation engines and methods. Many major platforms, such as Amazon, Netflix, and others, use these technologies. These recommendation engines use a machine learning system that takes into account user search results and preferences.</p>
<p>The algorithm uses this information to make similar recommendations the next time you open the platform.</p>
<p>You will receive notifications about new programs on Netflix. Netflix’s algorithm checks the entire viewing history of its subscribers. It uses this information to suggest new series based on the preferences of its millions of active viewers.</p>
<p>The same recommendation engine can also be used to create ads. Take Amazon, for example. Let us say you go to Amazon to store or just search for something. Amazon’s machine learning technology analyzes the user’s search results and then generates ads as recommendations.</p>
<ol class="simple">
<li><p><strong>Machine learning for Illness Prediction Healthcare use cases in healthcare</strong>.</p></li>
</ol>
<p><img alt="Building Heart disease classifier using K-NN algorithm: image from https://cdn-images-1.medium.com/max/800/1*tGeiO5zee6exueRC8iBuaQ.jpeg" src="https://cdn-images-1.medium.com/max/800/1*tGeiO5zee6exueRC8iBuaQ.jpeg" /></p>
<p>Doctors can warn patients ahead of time if they can predict a disease. They can even tell if a disease is dangerous or not, which is quite remarkable. But even though using ML is not an easy task, it can be of great benefit.</p>
<p>In this case, the ML algorithm first looks for symptoms on the patient’s body. It would use abnormal body functions as input, train the algorithm, and then make a prediction based on that. Since there are hundreds of diseases and twice as many symptoms, it may take some time to get the results.</p>
<ol class="simple">
<li><p><strong>Credit score - banking machine learning examples</strong>.</p></li>
</ol>
<p>It can be difficult to determine whether a bank customer is creditworthy. This is critical because whether or not the bank will grant you a loan depends on it.</p>
<p>Traditional credit card companies only check to see if the card is current and perform a history check. If the cardholder does not have a card history, the assessment becomes more difficult. For this, there are a number of machine learning algorithms that take into account the user’s financial situation, previous credit repayments, debts and so on.</p>
<p>Due to a large number of defaulters, banks have already suffered significant financial losses. To limit these types of losses, we need an effective machine learning system that can prevent any of these scenarios from occurring. This would save banks a lot of money and allow them to provide more services to real consumers.</p>
</section>
<section id="categories-of-machine-learning">
<h2><span class="section-number">9.3. </span>Categories of Machine Learning<a class="headerlink" href="#categories-of-machine-learning" title="Permalink to this headline">¶</a></h2>
<p>Machine learning can be divided into two forms at the most basic level: supervised learning and unsupervised learning.</p>
<section id="supervised-learning">
<h3><span class="section-number">9.3.1. </span>Supervised learning<a class="headerlink" href="#supervised-learning" title="Permalink to this headline">¶</a></h3>
<p>Supervised learning involves determining how to model the relationship between measured data features and a label associated with the data; once this model is determined, it can be used to apply labels to new, unknown data. This is further divided into <strong>classification</strong> and <strong>regression</strong> tasks, where the <strong>labels in classification are discrete categories</strong> and the <strong>labels in regression are continuous values</strong>.</p>
<section id="classification-problems-more-examples-below">
<h4><span class="section-number">9.3.1.1. </span>Classification problems (more examples below)<a class="headerlink" href="#classification-problems-more-examples-below" title="Permalink to this headline">¶</a></h4>
<p>The output of a classification task is a discrete value. “Likes adding sugar to coffee” and “does not like adding sugar to coffee,” for example, are discrete. There is no such thing as a middle ground. This is similar to teaching a child to recognize different types of animals, whether they are pets or not.</p>
<p>The output (label) of a classification method is typically represented as an integer number such as 1, -1, or 0. These figures are solely symbolic in this situation. Mathematical operations should not be performed with them because this would be pointless. Consider this for a moment. What is the difference between “Likes adding sugar to coffee” and “does not like adding sugar to coffee”? Exactly. We won’t be able to add them, therefore we won’t.</p>
</section>
<section id="regression-problem-discussed-in-our-last-chatpter">
<h4><span class="section-number">9.3.1.2. </span>Regression problem (discussed in our last chatpter)<a class="headerlink" href="#regression-problem-discussed-in-our-last-chatpter" title="Permalink to this headline">¶</a></h4>
<p>The outcome of a regression problem is a real number (a number with a decimal point). We could, for example, use the height and weight information to estimate someone’s weight based on their height.</p>
<p>The data for a regression analysis will like the data in insurance data set. A <strong>dependent variable</strong> (or set of independent variables) and an <strong>independent variable</strong> (the thing we are trying to guess given our independent variables) are both present.</p>
<p>We could state that height is the independent variable and weight is the dependent variable, for example.
In addition, each row in the dataset is commonly referred to as an <strong>example, observation, or data point</strong>, but each column (without the <strong>label/dependent variable</strong>) is commonly referred to as a <strong>predictor, independent variable, or feature</strong>.</p>
<p><img alt="Supervised learning: image from medium" src="https://miro.medium.com/max/1400/1*589X2eXJJkatGRG-z-s_oA.png" /></p>
</section>
</section>
<section id="unsupervised-learning-more-details-in-the-next-chapter">
<h3><span class="section-number">9.3.2. </span>Unsupervised learning (more details in the next chapter)<a class="headerlink" href="#unsupervised-learning-more-details-in-the-next-chapter" title="Permalink to this headline">¶</a></h3>
<p>Unsupervised learning, sometimes known as “letting the dataset speak for itself,” models the features of a dataset without reference to a label. Clustering and dimensionality reduction are among the tasks these models perform.</p>
<p><strong>Clustering methods</strong> find unique groups of data, while <strong>dimensionality reduction</strong> algorithms look for more concise representations.</p>
</section>
<section id="supervised-vs-unsupervised-learning-image-from-researchgate">
<h3><span class="section-number">9.3.3. </span>Supervised vs Unsupervised Learning: image from researchgate<a class="headerlink" href="#supervised-vs-unsupervised-learning-image-from-researchgate" title="Permalink to this headline">¶</a></h3>
<p><img alt="Supervised vs Unsupervised Learning: image from researchgate" src="https://www.researchgate.net/publication/329533120/figure/fig1/AS:702267594399761&#64;1544445050584/Supervised-learning-and-unsupervised-learning-Supervised-learning-uses-annotation_W640.jpg" /></p>
</section>
<section id="various-classification-regression-and-clustering-algorithms-image-from-scikit-learn">
<h3><span class="section-number">9.3.4. </span>Various classification, regression and clustering algorithms: image from scikit-learn<a class="headerlink" href="#various-classification-regression-and-clustering-algorithms-image-from-scikit-learn" title="Permalink to this headline">¶</a></h3>
<p><img alt="Various classification, regression and clustering algorithms: image from scikit-learn" src="https://upload.wikimedia.org/wikipedia/commons/thumb/a/a4/Scikit-learn_machine_learning_decision_tree.png/1024px-Scikit-learn_machine_learning_decision_tree.png" /></p>
</section>
</section>
<section id="machine-learning-basics-with-the-k-nearest-neighbors-algorithm">
<h2><span class="section-number">9.4. </span>Machine Learning Basics with the K-Nearest Neighbors Algorithm<a class="headerlink" href="#machine-learning-basics-with-the-k-nearest-neighbors-algorithm" title="Permalink to this headline">¶</a></h2>
<p>We will learn what <strong>K-nearest neighbours (KNN)</strong> is, how it works, and how to find the right k value. We will utilize the well-known Python library sklearn to demonstrate how to use KNN.</p>
<section id="k-nearest-neighbours-can-be-summarized-as-follows">
<h3><span class="section-number">9.4.1. </span>K-nearest neighbours can be summarized as follows:<a class="headerlink" href="#k-nearest-neighbours-can-be-summarized-as-follows" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>K- Nearest Neighbors is a <strong>supervised machine learning</strong> approach since the target variable is known,</p></li>
<li><p>It is <strong>non-parametric</strong>, since no assumptions are made about the underlying data distribution pattern.</p></li>
<li><p>It predicts the cluster into which the new point will fall based on feature similarity.</p></li>
</ul>
<p>Both classification and regression prediction problems can be solved with KNN. However, since most analytical problems require making a decision, it is more commonly used in classification problems .</p>
</section>
<section id="knn-algorithm-s-theory">
<h3><span class="section-number">9.4.2. </span>KNN algorithm’s theory<a class="headerlink" href="#knn-algorithm-s-theory" title="Permalink to this headline">¶</a></h3>
<p>The KNN algorithm’s concept is one of the most straightforward of all the supervised machine learning algorithms.</p>
<p>It simply calculates the distance between a new data point and all previous data points in the training set.</p>
<p>Any form of distance can be used, such as</p>
<ul class="simple">
<li><p>Euclidean or</p></li>
<li><p>Manhattan distances.</p></li>
</ul>
<p>The K-nearest data points are then chosen, where K can be any integer. Finally, the data point is assigned to the class that contains the majority of the K data points.</p>
<p>Note that the Manhattan distance, <span class="math notranslate nohighlight">\({\displaystyle d_{1}}\)</span>, between two vectors <span class="math notranslate nohighlight">\({\displaystyle \mathbf {p} ,\mathbf {q} }\)</span>  in an n-dimensional real vector space with fixed Cartesian coordinate system is defined as</p>
<div class="math notranslate nohighlight">
\[d_{1}(\mathbf {p} ,\mathbf {q} )=\|\mathbf {p} -\mathbf {q} \|_{1}=\sum _{i=1}^{n}|p_{i}-q_{i}|,\]</div>
<p>where <span class="math notranslate nohighlight">\({\displaystyle (\mathbf {p} ,\mathbf {q} )}\)</span> are vectors</p>
<div class="math notranslate nohighlight">
\[{\displaystyle \mathbf {p} =(p_{1},p_{2},\dots ,p_{n}){\text{ and }}\mathbf {q} =(q_{1},q_{2},\dots ,q_{n})\,}.\]</div>
</section>
<section id="example-on-knn-classifiers-image-from-researchgate">
<h3><span class="section-number">9.4.3. </span>Example on KNN classifiers: image from Researchgate<a class="headerlink" href="#example-on-knn-classifiers-image-from-researchgate" title="Permalink to this headline">¶</a></h3>
<p><img alt="Example on KNN classifiers: image from Researchgate" src="https://www.researchgate.net/profile/Mohammed-Badawy/publication/331424423/figure/fig1/AS:732056359297024&#64;1551547245072/Example-on-KNN-classifier_W640.jpg" /></p>
<p>Our goal in this diagram is to identify a new data point with the symbol ‘Pt’ into one of three categories: “A,” “B,” or “C.”</p>
<p>Assume that K is equal to 7. The KNN algorithm begins by computing the distance between point ‘Pt’ and all of the other points. The 7 closest points with the shortest distance to point ‘Pt’ are then found. This is depicted in the diagram below. Arrows have been used to denote the seven closest points.</p>
<p>The KNN algorithm’s final step is to assign a new point to the class that contains the majority of the seven closest points. Three of the seven closest points belong to the class “B,” while two of the seven belongs to the classes “A” and “C”. Therefore the new data point will be classified as “B”.</p>
</section>
<section id="dataset-https-raw-githubusercontent-com-susanli2016-machine-learning-with-python-master-fruit-data-with-colors-txt">
<h3><span class="section-number">9.4.4. </span>Dataset: <a class="reference external" href="https://raw.githubusercontent.com/susanli2016/Machine-Learning-with-Python/master/fruit_data_with_colors.txt">https://raw.githubusercontent.com/susanli2016/Machine-Learning-with-Python/master/fruit_data_with_colors.txt</a><a class="headerlink" href="#dataset-https-raw-githubusercontent-com-susanli2016-machine-learning-with-python-master-fruit-data-with-colors-txt" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="o">%</span><span class="k">matplotlib</span> inline
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">ModuleNotFoundError</span><span class="g g-Whitespace">                       </span>Traceback (most recent call last)
<span class="o">/</span><span class="n">var</span><span class="o">/</span><span class="n">folders</span><span class="o">/</span><span class="n">kl</span><span class="o">/</span><span class="n">h_r05n_j76n32kt0dwy7kynw0000gn</span><span class="o">/</span><span class="n">T</span><span class="o">/</span><span class="n">ipykernel_5515</span><span class="o">/</span><span class="mf">278679566.</span><span class="n">py</span> <span class="ow">in</span> <span class="o">&lt;</span><span class="n">module</span><span class="o">&gt;</span>
<span class="g g-Whitespace">      </span><span class="mi">3</span> <span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="g g-Whitespace">      </span><span class="mi">4</span> 
<span class="ne">----&gt; </span><span class="mi">5</span> <span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="g g-Whitespace">      </span><span class="mi">6</span> <span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="g g-Whitespace">      </span><span class="mi">7</span> <span class="n">get_ipython</span><span class="p">()</span><span class="o">.</span><span class="n">run_line_magic</span><span class="p">(</span><span class="s1">&#39;matplotlib&#39;</span><span class="p">,</span> <span class="s1">&#39;inline&#39;</span><span class="p">)</span>

<span class="ne">ModuleNotFoundError</span>: No module named &#39;seaborn&#39;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">plotnine</span> <span class="kn">import</span> <span class="o">*</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">warnings</span>

<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span> <span class="s2">&quot;ignore&quot;</span><span class="p">,</span> <span class="n">module</span> <span class="o">=</span> <span class="s2">&quot;matplotlib\..*&quot;</span> <span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We will be using the fruit_data_with_colors dataset, avalable here at github page, <a class="reference external" href="https://raw.githubusercontent.com/susanli2016/Machine-Learning-with-Python/master/fruit_data_with_colors.txt">https://raw.githubusercontent.com/susanli2016/Machine-Learning-with-Python/master/fruit_data_with_colors.txt</a>.</p>
<p>The mass, height, and width of a variety of oranges, lemons, and apples are included in the file. The heights were taken along the fruit’s core. The widths were measured perpendicular to the height at their widest point.</p>
<section id="our-goals">
<h4><span class="section-number">9.4.4.1. </span>Our goals<a class="headerlink" href="#our-goals" title="Permalink to this headline">¶</a></h4>
<p>To predict the appropriate fruit label, we’ll use the mass, width, and height of the fruit as our feature points (target value).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">url</span> <span class="o">=</span> <span class="s1">&#39;https://raw.githubusercontent.com/susanli2016/Machine-Learning-with-Python/master/fruit_data_with_colors.txt&#39;</span>
    
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_table</span><span class="p">(</span><span class="n">url</span><span class="p">)</span>    
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>fruit_label</th>
      <th>fruit_name</th>
      <th>fruit_subtype</th>
      <th>mass</th>
      <th>width</th>
      <th>height</th>
      <th>color_score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>apple</td>
      <td>granny_smith</td>
      <td>192</td>
      <td>8.4</td>
      <td>7.3</td>
      <td>0.55</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>apple</td>
      <td>granny_smith</td>
      <td>180</td>
      <td>8.0</td>
      <td>6.8</td>
      <td>0.59</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>apple</td>
      <td>granny_smith</td>
      <td>176</td>
      <td>7.4</td>
      <td>7.2</td>
      <td>0.60</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2</td>
      <td>mandarin</td>
      <td>mandarin</td>
      <td>86</td>
      <td>6.2</td>
      <td>4.7</td>
      <td>0.80</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2</td>
      <td>mandarin</td>
      <td>mandarin</td>
      <td>84</td>
      <td>6.0</td>
      <td>4.6</td>
      <td>0.79</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df</span><span class="o">.</span><span class="n">info</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
RangeIndex: 59 entries, 0 to 58
Data columns (total 7 columns):
 #   Column         Non-Null Count  Dtype  
---  ------         --------------  -----  
 0   fruit_label    59 non-null     int64  
 1   fruit_name     59 non-null     object 
 2   fruit_subtype  59 non-null     object 
 3   mass           59 non-null     int64  
 4   width          59 non-null     float64
 5   height         59 non-null     float64
 6   color_score    59 non-null     float64
dtypes: float64(3), int64(2), object(2)
memory usage: 3.4+ KB
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s1">&#39;fruit_label&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">count</span><span class="p">()[</span><span class="s1">&#39;fruit_name&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>fruit_label
1    19
2     5
3    19
4    16
Name: fruit_name, dtype: int64
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Fruit_label Count&#39;</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">countplot</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;fruit_name&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/Users/Kaemyuijang/opt/anaconda3/lib/python3.7/site-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;AxesSubplot:title={&#39;center&#39;:&#39;Fruit_label Count&#39;}, xlabel=&#39;fruit_name&#39;, ylabel=&#39;count&#39;&gt;
</pre></div>
</div>
<img alt="_images/Chapter7_Introduction_Machine_Learning_21_2.png" src="_images/Chapter7_Introduction_Machine_Learning_21_2.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>fruit_label</th>
      <th>fruit_name</th>
      <th>fruit_subtype</th>
      <th>mass</th>
      <th>width</th>
      <th>height</th>
      <th>color_score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>apple</td>
      <td>granny_smith</td>
      <td>192</td>
      <td>8.4</td>
      <td>7.3</td>
      <td>0.55</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>apple</td>
      <td>granny_smith</td>
      <td>180</td>
      <td>8.0</td>
      <td>6.8</td>
      <td>0.59</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>apple</td>
      <td>granny_smith</td>
      <td>176</td>
      <td>7.4</td>
      <td>7.2</td>
      <td>0.60</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2</td>
      <td>mandarin</td>
      <td>mandarin</td>
      <td>86</td>
      <td>6.2</td>
      <td>4.7</td>
      <td>0.80</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2</td>
      <td>mandarin</td>
      <td>mandarin</td>
      <td>84</td>
      <td>6.0</td>
      <td>4.6</td>
      <td>0.79</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="p">(</span>
    <span class="n">ggplot</span><span class="p">(</span><span class="n">df</span><span class="p">)</span> 
    <span class="o">+</span> <span class="n">aes</span><span class="p">(</span><span class="s1">&#39;fruit_name&#39;</span><span class="p">,</span><span class="n">fill</span> <span class="o">=</span> <span class="s1">&#39;fruit_name&#39;</span><span class="p">)</span> 
    <span class="o">+</span> <span class="n">geom_bar</span><span class="p">()</span>
    <span class="o">+</span> <span class="n">scale_fill_manual</span><span class="p">(</span><span class="n">values</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="s1">&#39;olivedrab&#39;</span><span class="p">,</span> <span class="s1">&#39;gold&#39;</span><span class="p">,</span> <span class="s1">&#39;orange&#39;</span><span class="p">])</span>

<span class="p">)</span>    
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/Chapter7_Introduction_Machine_Learning_23_0.png" src="_images/Chapter7_Introduction_Machine_Learning_23_0.png" />
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;ggplot: (305317857)&gt;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Check whether there are any missing values.</span>

<span class="n">df</span><span class="o">.</span><span class="n">isnull</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>fruit_label      0
fruit_name       0
fruit_subtype    0
mass             0
width            0
height           0
color_score      0
dtype: int64
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">fruit_label</span><span class="o">.</span><span class="n">unique</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">fruit_name</span><span class="o">.</span><span class="n">unique</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[1 2 3 4]
[&#39;apple&#39; &#39;mandarin&#39; &#39;orange&#39; &#39;lemon&#39;]
</pre></div>
</div>
</div>
</div>
<p>To make the results easier to understand, we first establish a mapping from fruit label value to fruit name.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># create a mapping from a mapping from fruit label value to fruit name</span>
<span class="n">lookup_fruit_name</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">fruit_label</span><span class="o">.</span><span class="n">unique</span><span class="p">(),</span> <span class="n">df</span><span class="o">.</span><span class="n">fruit_name</span><span class="o">.</span><span class="n">unique</span><span class="p">()))</span>
<span class="n">lookup_fruit_name</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{1: &#39;apple&#39;, 2: &#39;mandarin&#39;, 3: &#39;orange&#39;, 4: &#39;lemon&#39;}
</pre></div>
</div>
</div>
</div>
</section>
<section id="exploratory-data-analysis">
<h4><span class="section-number">9.4.4.2. </span>Exploratory data analysis<a class="headerlink" href="#exploratory-data-analysis" title="Permalink to this headline">¶</a></h4>
<p>It is now time to experiment with the data and make some visualizations.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sns</span><span class="o">.</span><span class="n">pairplot</span><span class="p">(</span><span class="n">df</span><span class="p">[[</span><span class="s1">&#39;fruit_name&#39;</span><span class="p">,</span><span class="s1">&#39;mass&#39;</span><span class="p">,</span><span class="s1">&#39;width&#39;</span><span class="p">,</span><span class="s1">&#39;height&#39;</span><span class="p">]],</span><span class="n">hue</span><span class="o">=</span><span class="s1">&#39;fruit_name&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;seaborn.axisgrid.PairGrid at 0x1232cd210&gt;
</pre></div>
</div>
<img alt="_images/Chapter7_Introduction_Machine_Learning_29_1.png" src="_images/Chapter7_Introduction_Machine_Learning_29_1.png" />
</div>
</div>
</section>
<section id="what-we-observe-from-the-figures">
<h4><span class="section-number">9.4.4.3. </span>What we observe from the figures?<a class="headerlink" href="#what-we-observe-from-the-figures" title="Permalink to this headline">¶</a></h4>
<ol class="simple">
<li><p>Mandarin has both lower mass and height. It also has the lower average widths.</p></li>
<li><p>Orange has higher average masses and widths.</p></li>
<li><p>There is a <strong>clear separation</strong> of lemon from the other both width-height plot and height-mass plot.</p></li>
<li><p>What else can we observe from the figures?</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s1">&#39;fruit_name&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">describe</span><span class="p">()[[</span><span class="s1">&#39;mass&#39;</span><span class="p">]]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead tr th {
        text-align: left;
    }

    .dataframe thead tr:last-of-type th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr>
      <th></th>
      <th colspan="8" halign="left">mass</th>
    </tr>
    <tr>
      <th></th>
      <th>count</th>
      <th>mean</th>
      <th>std</th>
      <th>min</th>
      <th>25%</th>
      <th>50%</th>
      <th>75%</th>
      <th>max</th>
    </tr>
    <tr>
      <th>fruit_name</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>apple</th>
      <td>19.0</td>
      <td>165.052632</td>
      <td>11.969747</td>
      <td>140.0</td>
      <td>156.0</td>
      <td>164.0</td>
      <td>172.0</td>
      <td>192.0</td>
    </tr>
    <tr>
      <th>lemon</th>
      <td>16.0</td>
      <td>150.000000</td>
      <td>37.487776</td>
      <td>116.0</td>
      <td>117.5</td>
      <td>131.0</td>
      <td>188.0</td>
      <td>216.0</td>
    </tr>
    <tr>
      <th>mandarin</th>
      <td>5.0</td>
      <td>81.200000</td>
      <td>3.898718</td>
      <td>76.0</td>
      <td>80.0</td>
      <td>80.0</td>
      <td>84.0</td>
      <td>86.0</td>
    </tr>
    <tr>
      <th>orange</th>
      <td>19.0</td>
      <td>193.789474</td>
      <td>73.635422</td>
      <td>140.0</td>
      <td>154.0</td>
      <td>160.0</td>
      <td>197.0</td>
      <td>362.0</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s1">&#39;fruit_name&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">describe</span><span class="p">()[[</span><span class="s1">&#39;height&#39;</span><span class="p">]]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead tr th {
        text-align: left;
    }

    .dataframe thead tr:last-of-type th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr>
      <th></th>
      <th colspan="8" halign="left">height</th>
    </tr>
    <tr>
      <th></th>
      <th>count</th>
      <th>mean</th>
      <th>std</th>
      <th>min</th>
      <th>25%</th>
      <th>50%</th>
      <th>75%</th>
      <th>max</th>
    </tr>
    <tr>
      <th>fruit_name</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>apple</th>
      <td>19.0</td>
      <td>7.342105</td>
      <td>0.291196</td>
      <td>6.8</td>
      <td>7.1</td>
      <td>7.3</td>
      <td>7.55</td>
      <td>7.9</td>
    </tr>
    <tr>
      <th>lemon</th>
      <td>16.0</td>
      <td>8.856250</td>
      <td>0.997977</td>
      <td>7.5</td>
      <td>8.1</td>
      <td>8.5</td>
      <td>9.80</td>
      <td>10.5</td>
    </tr>
    <tr>
      <th>mandarin</th>
      <td>5.0</td>
      <td>4.380000</td>
      <td>0.277489</td>
      <td>4.0</td>
      <td>4.3</td>
      <td>4.3</td>
      <td>4.60</td>
      <td>4.7</td>
    </tr>
    <tr>
      <th>orange</th>
      <td>19.0</td>
      <td>7.936842</td>
      <td>0.769712</td>
      <td>7.0</td>
      <td>7.4</td>
      <td>7.8</td>
      <td>8.15</td>
      <td>9.4</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s1">&#39;fruit_name&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">describe</span><span class="p">()[[</span><span class="s1">&#39;width&#39;</span><span class="p">]]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead tr th {
        text-align: left;
    }

    .dataframe thead tr:last-of-type th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr>
      <th></th>
      <th colspan="8" halign="left">width</th>
    </tr>
    <tr>
      <th></th>
      <th>count</th>
      <th>mean</th>
      <th>std</th>
      <th>min</th>
      <th>25%</th>
      <th>50%</th>
      <th>75%</th>
      <th>max</th>
    </tr>
    <tr>
      <th>fruit_name</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>apple</th>
      <td>19.0</td>
      <td>7.457895</td>
      <td>0.345311</td>
      <td>6.9</td>
      <td>7.3</td>
      <td>7.4</td>
      <td>7.600</td>
      <td>8.4</td>
    </tr>
    <tr>
      <th>lemon</th>
      <td>16.0</td>
      <td>6.512500</td>
      <td>0.624900</td>
      <td>5.8</td>
      <td>6.0</td>
      <td>6.2</td>
      <td>7.225</td>
      <td>7.3</td>
    </tr>
    <tr>
      <th>mandarin</th>
      <td>5.0</td>
      <td>5.940000</td>
      <td>0.167332</td>
      <td>5.8</td>
      <td>5.8</td>
      <td>5.9</td>
      <td>6.000</td>
      <td>6.2</td>
    </tr>
    <tr>
      <th>orange</th>
      <td>19.0</td>
      <td>7.557895</td>
      <td>0.813986</td>
      <td>6.7</td>
      <td>7.1</td>
      <td>7.2</td>
      <td>7.600</td>
      <td>9.6</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
</section>
<section id="preprocessing-train-test-split">
<h4><span class="section-number">9.4.4.4. </span>Preprocessing: Train Test Split.<a class="headerlink" href="#preprocessing-train-test-split" title="Permalink to this headline">¶</a></h4>
<p>Because training and testing on the same data is inefficient, we partition the data into two sets: <strong>training and testing</strong>.</p>
<p>To split the data, we use the <code class="docutils literal notranslate"><span class="pre">train_test_split</span></code> function.</p>
<p>The split percentage is determined by the optional parameter <code class="docutils literal notranslate"><span class="pre">test_size.</span></code> The default values are 75/25% train and test data.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">random</span> <span class="pre">state</span></code> parameter ensures that the data is split in the same way each time the program is executed.</p>
<p>Because we are training and testing on distinct sets of data, the testing accuracy will be a better indication of how well the model will perform on new data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Train Test Split</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="p">[[</span><span class="s1">&#39;height&#39;</span><span class="p">,</span> <span class="s1">&#39;width&#39;</span><span class="p">,</span> <span class="s1">&#39;mass&#39;</span><span class="p">]]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;fruit_label&#39;</span><span class="p">]</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#shape of train and test data</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X_test</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(44, 3)
(15, 3)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#shape of new y objects</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y_train</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y_test</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(44,)
(15,)
</pre></div>
</div>
</div>
</div>
</section>
<section id="training-and-predictions">
<h4><span class="section-number">9.4.4.5. </span>Training and Predictions<a class="headerlink" href="#training-and-predictions" title="Permalink to this headline">¶</a></h4>
<p>Scikit-learn is divided into modules so that we may quickly import the classes we need.</p>
<p>Import the <code class="docutils literal notranslate"><span class="pre">KNeighborsClassifer</span></code> class from the <code class="docutils literal notranslate"><span class="pre">neighbors</span></code> module.</p>
<p>Instantiate the estimator (a model in scikit-learn is referred to as a <strong>estimator</strong>). Because their major function is to estimate unknown quantities, we refer to the model as an estimator.</p>
<p>In our example, we have generated an instance (<code class="docutils literal notranslate"><span class="pre">knn</span></code>) of the class <code class="docutils literal notranslate"><span class="pre">KNeighborsClassifer</span></code>, which means we have constructed an object called ‘knn’ that knows how to perform KNN classification once the data is provided.</p>
<p>The <strong>tuning parameter/hyper parameter</strong> (k) is the parameter <code class="docutils literal notranslate"><span class="pre">n_</span> <span class="pre">neighbors</span></code>. All other parameters are set to default.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">fit</span></code> method is used to train the model using training data (X train,y train), while the <code class="docutils literal notranslate"><span class="pre">predict</span></code> method is used to test the model using testing data (X test).</p>
<p>In this example, we take <code class="docutils literal notranslate"><span class="pre">n_neighbors</span></code> or k = 5.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>

<span class="n">knn</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span> <span class="o">=</span> <span class="mi">5</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="train-the-classifier-fit-the-estimator-using-the-training-data">
<h4><span class="section-number">9.4.4.6. </span>Train the classifier (fit the estimator) using the training data¶<a class="headerlink" href="#train-the-classifier-fit-the-estimator-using-the-training-data" title="Permalink to this headline">¶</a></h4>
<p>We then train the classifier by passing the training set data in <strong>X_train</strong> and the labels in <strong>y_train</strong> to the classifier’s fit method.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">knn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>KNeighborsClassifier()
</pre></div>
</div>
</div>
</div>
</section>
<section id="estimate-the-accuracy-of-the-classifier-on-future-data-using-the-test-data">
<h4><span class="section-number">9.4.4.7. </span>Estimate the accuracy of the classifier on future data, using the test data<a class="headerlink" href="#estimate-the-accuracy-of-the-classifier-on-future-data-using-the-test-data" title="Permalink to this headline">¶</a></h4>
<p>Remember that the KNN classifier has not seen any of the fruits in the <strong>test set</strong> during the training phase.</p>
<p>To do this, we use the score method for the classifier object.
This takes the points in the test set as input and calculates the <strong>accuracy</strong>.</p>
<p>The <strong>accuracy</strong> is defined as the proportion of points in the test set whose true label was correctly predicted by the classifier.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">knn</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Accuracy:&#39;</span><span class="p">,</span> <span class="n">knn</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Accuracy: 0.5333333333333333
</pre></div>
</div>
</div>
</div>
<p>We obtain a classficiation rate of 53.3%, considered as good accuracy.</p>
<p>Can we further improve the accuracy of the KNN algorithm?</p>
</section>
<section id="use-the-trained-knn-classifier-model-to-classify-new-previously-unseen-objects">
<h4><span class="section-number">9.4.4.8. </span>Use the trained KNN classifier model to classify new, previously unseen objects<a class="headerlink" href="#use-the-trained-knn-classifier-model-to-classify-new-previously-unseen-objects" title="Permalink to this headline">¶</a></h4>
<p>So, here for example. We are entering the mass, width, and height for a hypothetical piece of fruit that is pretty small.</p>
<p>And if we ask the classifier to predict the label using the predict method.</p>
<p>We can see that the output says that it is a mandarin.</p>
<p>for example: a small fruit with a mass of 20 g, a width of 4.5 cm and a height of 5.2 cm</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_train</span><span class="o">.</span><span class="n">columns</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Index([&#39;height&#39;, &#39;width&#39;, &#39;mass&#39;], dtype=&#39;object&#39;)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#sample1 = pd.DataFrame({&#39;height&#39;:[5.2], &#39;width&#39;:[4.5],&#39;mass&#39;:[20]})</span>
<span class="c1"># Notice we use the same column as the X training data (or X test sate)</span>
<span class="n">sample1</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">([[</span><span class="mf">5.2</span><span class="p">,</span><span class="mf">4.5</span><span class="p">,</span><span class="mi">20</span><span class="p">]],</span><span class="n">columns</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
<span class="n">fruit_prediction</span> <span class="o">=</span> <span class="n">knn</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">sample1</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;The prediction is:&#39;</span><span class="p">,</span> <span class="n">lookup_fruit_name</span><span class="p">[</span><span class="n">fruit_prediction</span><span class="p">[</span><span class="mi">0</span><span class="p">]])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The prediction is: mandarin
</pre></div>
</div>
</div>
</div>
<p>Here another example</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#sample2 = pd.DataFrame({&#39;height&#39;:[6.8], &#39;width&#39;:[8.5],&#39;mass&#39;:[180]})</span>

<span class="n">sample2</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">([[</span><span class="mf">6.8</span><span class="p">,</span><span class="mf">8.5</span><span class="p">,</span><span class="mi">180</span><span class="p">]],</span><span class="n">columns</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
<span class="n">fruit_prediction</span> <span class="o">=</span> <span class="n">knn</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">sample2</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;The prediction is:&#39;</span><span class="p">,</span> <span class="n">lookup_fruit_name</span><span class="p">[</span><span class="n">fruit_prediction</span><span class="p">[</span><span class="mi">0</span><span class="p">]])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The prediction is: apple
</pre></div>
</div>
</div>
</div>
<p><strong>Exercise</strong></p>
<ol class="simple">
<li><p>Create a DataFrame comparing the y_test and the predictions from the model.</p></li>
<li><p>Confirm that the accuracy is the same as obtained by knn.score(X_test, y_test).</p></li>
</ol>
<p>Here is the list of predictions of the test set obtained from the model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#fruit_prediction = knn.predict([[20,4.5,5.2]])</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">knn</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="calculating-the-agorithm-accuracy">
<h4><span class="section-number">9.4.4.9. </span>Calculating the agorithm accuracy<a class="headerlink" href="#calculating-the-agorithm-accuracy" title="Permalink to this headline">¶</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># the accuracy of the test set</span>

<span class="c1"># https://stackoverflow.com/questions/59072143/pandas-mean-of-boolean</span>
<span class="p">((</span><span class="n">knn</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span> <span class="o">==</span> <span class="n">y_test</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">())</span><span class="o">*</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="p">(</span><span class="n">knn</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span> <span class="o">==</span> <span class="n">y_test</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">())</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.5333333333333333
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># the accuracy of the original data set</span>

<span class="c1"># (knn.predict(X) == y.to_numpy()).mean()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># the accuracy of the training set</span>

<span class="p">(</span><span class="n">knn</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span> <span class="o">==</span> <span class="n">y_train</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">())</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.7954545454545454
</pre></div>
</div>
</div>
</div>
</section>
<section id="comparison-of-the-observed-and-predicted-values-from-both-training-and-test-data-sets">
<h4><span class="section-number">9.4.4.10. </span>Comparison of the observed and predicted values from both training and test data sets<a class="headerlink" href="#comparison-of-the-observed-and-predicted-values-from-both-training-and-test-data-sets" title="Permalink to this headline">¶</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Printing out the observed and predicted values of the test set</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X_test</span><span class="p">)):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Observed: &#39;</span><span class="p">,</span> <span class="n">lookup_fruit_name</span><span class="p">[</span><span class="n">y_test</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">i</span><span class="p">]],</span> <span class="s1">&#39;vs Predicted:&#39;</span><span class="p">,</span>    <span class="n">lookup_fruit_name</span><span class="p">[</span><span class="n">knn</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[</span><span class="n">i</span><span class="p">]])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Observed:  orange vs Predicted: orange
Observed:  orange vs Predicted: apple
Observed:  lemon vs Predicted: lemon
Observed:  orange vs Predicted: lemon
Observed:  apple vs Predicted: apple
Observed:  apple vs Predicted: apple
Observed:  orange vs Predicted: orange
Observed:  lemon vs Predicted: orange
Observed:  orange vs Predicted: apple
Observed:  apple vs Predicted: lemon
Observed:  mandarin vs Predicted: mandarin
Observed:  apple vs Predicted: apple
Observed:  orange vs Predicted: orange
Observed:  orange vs Predicted: apple
Observed:  orange vs Predicted: lemon
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">X_test</span><span class="o">.</span><span class="n">index</span><span class="p">)</span>
<span class="n">y_test</span><span class="o">.</span><span class="n">index</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Int64Index([26, 35, 43, 28, 11, 2, 34, 46, 40, 22, 4, 10, 30, 41, 33], dtype=&#39;int64&#39;)
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Int64Index([26, 35, 43, 28, 11, 2, 34, 46, 40, 22, 4, 10, 30, 41, 33], dtype=&#39;int64&#39;)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;observed&#39;</span><span class="p">:</span> <span class="n">y_test</span> <span class="p">,</span><span class="s1">&#39;predicted&#39;</span><span class="p">:</span><span class="n">knn</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)})</span><span class="o">.</span><span class="n">set_index</span><span class="p">(</span><span class="n">X_test</span><span class="o">.</span><span class="n">index</span><span class="p">)</span> 
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>observed</th>
      <th>predicted</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>26</th>
      <td>3</td>
      <td>3</td>
    </tr>
    <tr>
      <th>35</th>
      <td>3</td>
      <td>1</td>
    </tr>
    <tr>
      <th>43</th>
      <td>4</td>
      <td>4</td>
    </tr>
    <tr>
      <th>28</th>
      <td>3</td>
      <td>4</td>
    </tr>
    <tr>
      <th>11</th>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>34</th>
      <td>3</td>
      <td>3</td>
    </tr>
    <tr>
      <th>46</th>
      <td>4</td>
      <td>3</td>
    </tr>
    <tr>
      <th>40</th>
      <td>3</td>
      <td>1</td>
    </tr>
    <tr>
      <th>22</th>
      <td>1</td>
      <td>4</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>10</th>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>30</th>
      <td>3</td>
      <td>3</td>
    </tr>
    <tr>
      <th>41</th>
      <td>3</td>
      <td>1</td>
    </tr>
    <tr>
      <th>33</th>
      <td>3</td>
      <td>4</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;observed&#39;</span><span class="p">:</span> <span class="n">y_train</span> <span class="p">,</span><span class="s1">&#39;predicted&#39;</span><span class="p">:</span><span class="n">knn</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)})</span><span class="o">.</span><span class="n">set_index</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">index</span><span class="p">)</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>observed</th>
      <th>predicted</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>42</th>
      <td>3</td>
      <td>1</td>
    </tr>
    <tr>
      <th>48</th>
      <td>4</td>
      <td>1</td>
    </tr>
    <tr>
      <th>7</th>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>14</th>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>32</th>
      <td>3</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;observed&#39;</span><span class="p">:</span> <span class="n">y</span> <span class="p">,</span><span class="s1">&#39;predicted&#39;</span><span class="p">:</span><span class="n">knn</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)})</span><span class="o">.</span><span class="n">set_index</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">index</span><span class="p">)</span><span class="o">.</span><span class="n">head</span><span class="p">()</span> 
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>observed</th>
      <th>predicted</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>4</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df_result</span> <span class="o">=</span> <span class="n">df</span>
<span class="n">df_result</span><span class="p">[</span><span class="s1">&#39;predicted_label&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">knn</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">df_result</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>fruit_label</th>
      <th>fruit_name</th>
      <th>fruit_subtype</th>
      <th>mass</th>
      <th>width</th>
      <th>height</th>
      <th>color_score</th>
      <th>predicted_label</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>apple</td>
      <td>granny_smith</td>
      <td>192</td>
      <td>8.4</td>
      <td>7.3</td>
      <td>0.55</td>
      <td>4</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>apple</td>
      <td>granny_smith</td>
      <td>180</td>
      <td>8.0</td>
      <td>6.8</td>
      <td>0.59</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>apple</td>
      <td>granny_smith</td>
      <td>176</td>
      <td>7.4</td>
      <td>7.2</td>
      <td>0.60</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2</td>
      <td>mandarin</td>
      <td>mandarin</td>
      <td>86</td>
      <td>6.2</td>
      <td>4.7</td>
      <td>0.80</td>
      <td>2</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2</td>
      <td>mandarin</td>
      <td>mandarin</td>
      <td>84</td>
      <td>6.0</td>
      <td>4.6</td>
      <td>0.79</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
</section>
<section id="visualize-the-decision-regions-of-a-classifier">
<h4><span class="section-number">9.4.4.11. </span>Visualize the decision regions of a classifier<a class="headerlink" href="#visualize-the-decision-regions-of-a-classifier" title="Permalink to this headline">¶</a></h4>
<p>After a classifier has been trained on training data, a classification model is created. What criteria does your machine learning classifier consider when deciding which class a sample belongs to? Plotting a decision region can provide some insight into the decision made by your ML classifier.</p>
<p>A <strong>decision region</strong> is a region in which a classifier predicts the same class label for data.</p>
<p>The boundary between areas of various classes is known as the <strong>decision boundary</strong>.</p>
<p>The plot decision regions function in <code class="docutils literal notranslate"><span class="pre">mlxtend</span></code> is a simple way to plot decision areas. We can also use mlxtend to plot  decision regions of <strong>Logistic Regression, Random Forest, RBF kernel SVM, and Ensemble classifier</strong>.</p>
<p><strong>Important Note</strong> for the 2D scatterplot, we can <strong>only visualize 2 features</strong> at a time. So, if you have a 3-dimensional (or more than 3) dataset, it will essentially be a <strong>2D slice through this feature space with fixed values for the remaining feature(s)</strong>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mlxtend.plotting</span> <span class="kn">import</span> <span class="n">plot_decision_regions</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Importing a dataset</span>
<span class="n">url</span> <span class="o">=</span> <span class="s1">&#39;https://raw.githubusercontent.com/susanli2016/Machine-Learning-with-Python/master/fruit_data_with_colors.txt&#39;</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_table</span><span class="p">(</span><span class="n">url</span><span class="p">)</span>  

<span class="c1"># Train Test Split</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="p">[[</span><span class="s1">&#39;height&#39;</span><span class="p">,</span> <span class="s1">&#39;width&#39;</span><span class="p">,</span> <span class="s1">&#39;mass&#39;</span><span class="p">]]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;fruit_label&#39;</span><span class="p">]</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Instantiate the estimator</span>
<span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>
<span class="n">knn</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span> <span class="o">=</span> <span class="mi">5</span><span class="p">)</span>

<span class="c1"># Training the classifier by passing in the training set X_train and the labels in y_train</span>
<span class="n">knn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Predicting labels for unknown data</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">knn</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_train</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>height</th>
      <th>width</th>
      <th>mass</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>44.000000</td>
      <td>44.000000</td>
      <td>44.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>7.643182</td>
      <td>7.038636</td>
      <td>159.090909</td>
    </tr>
    <tr>
      <th>std</th>
      <td>1.370350</td>
      <td>0.835886</td>
      <td>53.316876</td>
    </tr>
    <tr>
      <th>min</th>
      <td>4.000000</td>
      <td>5.800000</td>
      <td>76.000000</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>7.200000</td>
      <td>6.175000</td>
      <td>127.500000</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>7.600000</td>
      <td>7.200000</td>
      <td>157.000000</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>8.250000</td>
      <td>7.500000</td>
      <td>172.500000</td>
    </tr>
    <tr>
      <th>max</th>
      <td>10.500000</td>
      <td>9.200000</td>
      <td>356.000000</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
</section>
<section id="decision-regions-for-two-features-height-and-mass">
<h4><span class="section-number">9.4.4.12. </span>Decision regions for two features, height and mass.<a class="headerlink" href="#decision-regions-for-two-features-height-and-mass" title="Permalink to this headline">¶</a></h4>
<p>We may want to go beyond the numerical prediction (of the class or of the probability) and visualize the actual decision boundaries between the classes for many classification problems in the domain of supervised ML.</p>
<p>The visualization is displayed on a <strong>2-dimensional (2D) plane</strong>, which makes it particularly ideal for <strong>binary classification</strong> tasks and for <strong>a pair of features</strong>.</p>
<p>We will be using the <code class="docutils literal notranslate"><span class="pre">plot_decision_regions()</span></code> function in the <code class="docutils literal notranslate"><span class="pre">MLxtend</span></code> library to draw a classifier’s decision regions.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">knn</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span> <span class="o">=</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">knn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">y_train</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>

<span class="c1"># Plotting decision regions</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>

<span class="c1"># Decision region for feature 2 = width = value</span>
<span class="n">value</span><span class="o">=</span><span class="mi">7</span>

<span class="c1"># Plot training sample points with </span>
<span class="c1"># feature 2 = width = value +/- width</span>
<span class="n">width</span><span class="o">=</span><span class="mf">0.3</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plot_decision_regions</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">y_train</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">clf</span><span class="o">=</span><span class="n">knn</span><span class="p">,</span>
              <span class="n">feature_index</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">],</span>   <span class="c1">#these one will be plotted  </span>
              <span class="n">filler_feature_values</span><span class="o">=</span><span class="p">{</span><span class="mi">1</span><span class="p">:</span> <span class="n">value</span><span class="p">},</span>  <span class="c1">#these will be ignored</span>
              <span class="n">filler_feature_ranges</span><span class="o">=</span><span class="p">{</span><span class="mi">1</span><span class="p">:</span> <span class="n">width</span><span class="p">})</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Feature 1 (height)&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Feature 2 (mass)&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Feature 3 (width) = </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">value</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/Users/Kaemyuijang/opt/anaconda3/lib/python3.7/site-packages/mlxtend/plotting/decision_regions.py:279: UserWarning: You passed a edgecolor/edgecolors (&#39;black&#39;) for an unfilled marker (&#39;x&#39;).  Matplotlib is ignoring the edgecolor in favor of the facecolor.  This behavior may change in the future.
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0.5, 1.0, &#39;Feature 3 (width) = 7&#39;)
</pre></div>
</div>
<img alt="_images/Chapter7_Introduction_Machine_Learning_71_2.png" src="_images/Chapter7_Introduction_Machine_Learning_71_2.png" />
</div>
</div>
<p><strong>Important Notes</strong>: the arguments used in the <code class="docutils literal notranslate"><span class="pre">plot_decision_regions</span></code> are explained below:</p>
<ol class="simple">
<li><p><strong>feature_index</strong>: feature indices to use for plotting.</p></li>
</ol>
<p>This arguement defines the indices to be displayed in the 2D plot of the decision boundary. The first index in feature_index will be on the x-axis, the second index will be on the y-axis. (for array-like (default: (0,) for 1D, (0, 1) otherwise))</p>
<ol class="simple">
<li><p><strong>filler_feature_values</strong>:</p></li>
</ol>
<p>This arguement defines <strong>the indices and their (fixed) values</strong> of the features that are not included in the 2D plot of the decision boundary (index-value pairs for the features that are not displayed). Required only for Number Features &gt; 2.</p>
<p>The function ‘plot_decision_regions’ fits the data and makes a prediction to create the appropriate decision boundary by finding the predicted value for each point (the values of the ignored features are <strong>fixed</strong> and equal to the values specified by the user) in a grid-like scatter plot.</p>
<ol class="simple">
<li><p><strong>filler_feature_ranges</strong>:</p></li>
</ol>
<p>The last argument we included for the ‘plot_decision_regions’ function is ‘filler_feature_ranges’. This arguement defines the ranges of features that will not be plotted, and these regions are used to select (training) sample points for plotting.</p>
<p>The following Python gives sample points that are plotted in the above decision region, i.e. the range of the widths is in this interval (value - width, value + width).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Points to be included in the plot above, </span>
<span class="c1"># i.e. those sample points with width between (value - width, value + width)</span>

<span class="c1"># X_train</span>

<span class="n">X_train</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
<span class="c1">#X_train.head()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X_train</span><span class="p">[(</span><span class="n">X_train</span><span class="o">.</span><span class="n">width</span> <span class="o">&lt;</span> <span class="n">value</span> <span class="o">+</span> <span class="n">width</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">width</span> <span class="o">&gt;</span> <span class="n">value</span> <span class="o">-</span> <span class="n">width</span><span class="p">)]</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span> <span class="o">=</span> <span class="s1">&#39;height&#39;</span><span class="p">))</span>
<span class="c1">#print(y_train[(X_train.width &lt; value + width) &amp; (X_train.width &gt; value - width)])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>    height  width  mass
32     7.0    7.2   164
12     7.1    7.0   154
42     7.2    7.2   154
29     7.4    7.0   160
39     7.4    6.8   144
36     7.6    7.1   160
8      7.8    7.1   178
38     7.8    7.2   158
45     9.2    7.2   186
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#df.head()</span>
<span class="c1">#X_train.head()</span>
<span class="c1">#df[(df.width &lt; 7.2) &amp; (df.width &gt; 6.8)].sort_values(by = &#39;height&#39;)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="decision-regions-for-two-features-height-and-width">
<h4><span class="section-number">9.4.4.13. </span>Decision regions for two features, height and width.<a class="headerlink" href="#decision-regions-for-two-features-height-and-width" title="Permalink to this headline">¶</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#X_train.describe()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">knn</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span> <span class="o">=</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">knn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">y_train</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>

<span class="c1"># Plotting decision regions</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>

<span class="c1"># Decision region for feature 3 = mass = value</span>
<span class="n">value</span><span class="o">=</span><span class="mi">160</span>
<span class="c1"># Plot training sample with feature 3 = mass = value +/- width</span>
<span class="n">width</span><span class="o">=</span><span class="mi">30</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plot_decision_regions</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">y_train</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">clf</span><span class="o">=</span><span class="n">knn</span><span class="p">,</span>
              <span class="n">feature_index</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span>               <span class="c1">#these one will be plotted  </span>
              <span class="n">filler_feature_values</span><span class="o">=</span><span class="p">{</span><span class="mi">2</span><span class="p">:</span> <span class="n">value</span><span class="p">},</span>  <span class="c1">#these will be ignored</span>
              <span class="n">filler_feature_ranges</span><span class="o">=</span><span class="p">{</span><span class="mi">2</span><span class="p">:</span> <span class="n">width</span><span class="p">})</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Feature 1 (height)&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Feature 2 (width)&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Feature 3 (mass) = </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">value</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/Users/Kaemyuijang/opt/anaconda3/lib/python3.7/site-packages/mlxtend/plotting/decision_regions.py:279: UserWarning: You passed a edgecolor/edgecolors (&#39;black&#39;) for an unfilled marker (&#39;x&#39;).  Matplotlib is ignoring the edgecolor in favor of the facecolor.  This behavior may change in the future.
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0.5, 1.0, &#39;Feature 3 (mass) = 160&#39;)
</pre></div>
</div>
<img alt="_images/Chapter7_Introduction_Machine_Learning_78_2.png" src="_images/Chapter7_Introduction_Machine_Learning_78_2.png" />
</div>
</div>
<p><strong>Important Note</strong>: Do not be surprised that the samples (with the cross symbol) are correctly classified. Why?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#print(X_train.head())</span>
<span class="c1">#print(y_train)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">##### Convert pandas DataFrame to Numpy before applying classification</span>
<span class="c1">##### </span>
<span class="c1">#### weights{‘uniform’, ‘distance’} </span>

<span class="c1">#### ‘uniform’ : uniform weights. All points in each neighborhood are weighted equally.</span>

<span class="c1">#### ‘distance’ : weight points by the inverse of their distance. in this case, closer neighbors of a query point will have a greater influence than neighbors which are further away.</span>


<span class="n">X_train_np</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>
<span class="n">y_train_np</span> <span class="o">=</span> <span class="n">y_train</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>


<span class="n">clf</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span><span class="n">weights</span><span class="o">=</span><span class="s1">&#39;distance&#39;</span><span class="p">)</span>
<span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_np</span><span class="p">,</span> <span class="n">y_train_np</span><span class="p">)</span>

<span class="c1"># Plotting decision regions</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>

<span class="c1"># Decision region for feature 3 = mass = value</span>
<span class="n">value</span><span class="o">=</span><span class="mi">160</span>
<span class="c1"># Plot training sample with feature = mass = value +/- width</span>
<span class="n">width</span><span class="o">=</span><span class="mi">100</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plot_decision_regions</span><span class="p">(</span><span class="n">X_train_np</span><span class="p">,</span> <span class="n">y_train_np</span><span class="p">,</span> <span class="n">clf</span><span class="o">=</span><span class="n">clf</span><span class="p">,</span>
              <span class="n">feature_index</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span>               <span class="c1">#these one will be plotted  </span>
              <span class="n">filler_feature_values</span><span class="o">=</span><span class="p">{</span><span class="mi">2</span><span class="p">:</span> <span class="n">value</span><span class="p">},</span>  <span class="c1">#these will be ignored</span>
              <span class="n">filler_feature_ranges</span><span class="o">=</span><span class="p">{</span><span class="mi">2</span><span class="p">:</span> <span class="n">width</span><span class="p">})</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Feature 1 (height)&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Feature 2 (width)&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Feature 3 (mass) = </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">value</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/Users/Kaemyuijang/opt/anaconda3/lib/python3.7/site-packages/mlxtend/plotting/decision_regions.py:279: UserWarning: You passed a edgecolor/edgecolors (&#39;black&#39;) for an unfilled marker (&#39;x&#39;).  Matplotlib is ignoring the edgecolor in favor of the facecolor.  This behavior may change in the future.
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0.5, 1.0, &#39;Feature 3 (mass) = 160&#39;)
</pre></div>
</div>
<img alt="_images/Chapter7_Introduction_Machine_Learning_81_2.png" src="_images/Chapter7_Introduction_Machine_Learning_81_2.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># clf.predict(X_train_np)</span>
<span class="c1"># y_train_np</span>
<span class="c1"># X_train.describe()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># X_train_np = X_train.iloc[:,0:2].to_numpy()</span>
<span class="c1"># y_train_np = y_train.to_numpy()</span>

<span class="c1"># X_train_np.shape</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="visualization-of-dicision-boundary-with-knn-classification-for-a-dataset-with-only-two-features">
<h4><span class="section-number">9.4.4.14. </span>Visualization of Dicision Boundary with KNN classification for a dataset with only two features.<a class="headerlink" href="#visualization-of-dicision-boundary-with-knn-classification-for-a-dataset-with-only-two-features" title="Permalink to this headline">¶</a></h4>
<p>We will visualize the actual decision boundaries by training the KNN model with the dataset consisting of only one pair of features. We will then make a comparison the decision boundaries with the previous results.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">##### Convert pandas DataFrame to Numpy before applying classification</span>

<span class="n">X_train_np</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="mi">0</span><span class="p">:</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>
<span class="n">y_train_np</span> <span class="o">=</span> <span class="n">y_train</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>


<span class="n">clf</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span> <span class="o">=</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_np</span><span class="p">,</span> <span class="n">y_train_np</span><span class="p">)</span>

<span class="c1"># Plotting decision regions</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>

<span class="c1"># Decision region for feature 3 = mass = value</span>
<span class="c1"># value=160</span>
<span class="c1"># Plot training sample with feature = mass = value +/- width</span>
<span class="c1"># width=20</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plot_decision_regions</span><span class="p">(</span><span class="n">X_train_np</span><span class="p">,</span> <span class="n">y_train_np</span><span class="p">,</span> <span class="n">clf</span><span class="o">=</span><span class="n">clf</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Feature 1 (height)&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Feature 2 (width)&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Classification problem&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/Users/Kaemyuijang/opt/anaconda3/lib/python3.7/site-packages/mlxtend/plotting/decision_regions.py:279: UserWarning: You passed a edgecolor/edgecolors (&#39;black&#39;) for an unfilled marker (&#39;x&#39;).  Matplotlib is ignoring the edgecolor in favor of the facecolor.  This behavior may change in the future.
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0.5, 1.0, &#39;Classification problem&#39;)
</pre></div>
</div>
<img alt="_images/Chapter7_Introduction_Machine_Learning_85_2.png" src="_images/Chapter7_Introduction_Machine_Learning_85_2.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">##### Convert pandas DataFrame to Numpy before applying classification</span>

<span class="n">X_train_np</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,[</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">]]</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>
<span class="n">y_train_np</span> <span class="o">=</span> <span class="n">y_train</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>


<span class="n">clf</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span> <span class="o">=</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_np</span><span class="p">,</span> <span class="n">y_train_np</span><span class="p">)</span>

<span class="c1"># Plotting decision regions</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>

<span class="c1"># Decision region for feature 3 = mass = value</span>
<span class="n">value</span><span class="o">=</span><span class="mi">160</span>
<span class="c1"># Plot training sample with feature = mass = value +/- width</span>
<span class="n">width</span><span class="o">=</span><span class="mi">20</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plot_decision_regions</span><span class="p">(</span><span class="n">X_train_np</span><span class="p">,</span> <span class="n">y_train_np</span><span class="p">,</span> <span class="n">clf</span><span class="o">=</span><span class="n">clf</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Feature 1 (height)&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Feature 2 (mass)&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Classification problem&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/Users/Kaemyuijang/opt/anaconda3/lib/python3.7/site-packages/mlxtend/plotting/decision_regions.py:279: UserWarning: You passed a edgecolor/edgecolors (&#39;black&#39;) for an unfilled marker (&#39;x&#39;).  Matplotlib is ignoring the edgecolor in favor of the facecolor.  This behavior may change in the future.
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0.5, 1.0, &#39;Classification problem&#39;)
</pre></div>
</div>
<img alt="_images/Chapter7_Introduction_Machine_Learning_86_2.png" src="_images/Chapter7_Introduction_Machine_Learning_86_2.png" />
</div>
</div>
</section>
<section id="width-mass-visualization">
<h4><span class="section-number">9.4.4.15. </span>Width-Mass visualization<a class="headerlink" href="#width-mass-visualization" title="Permalink to this headline">¶</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#print(X_train.head())</span>
<span class="c1">#print(X_train.values)</span>
<span class="c1">#X_train.to_numpy()</span>

<span class="c1">#print(type(X_train.values))</span>
<span class="c1">#print(type(X_train.to_numpy()))</span>

<span class="c1">#print(X_train.values.shape)</span>
<span class="c1">#print(X_train.to_numpy().shape)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_train</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>height</th>
      <th>width</th>
      <th>mass</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>44.000000</td>
      <td>44.000000</td>
      <td>44.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>7.643182</td>
      <td>7.038636</td>
      <td>159.090909</td>
    </tr>
    <tr>
      <th>std</th>
      <td>1.370350</td>
      <td>0.835886</td>
      <td>53.316876</td>
    </tr>
    <tr>
      <th>min</th>
      <td>4.000000</td>
      <td>5.800000</td>
      <td>76.000000</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>7.200000</td>
      <td>6.175000</td>
      <td>127.500000</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>7.600000</td>
      <td>7.200000</td>
      <td>157.000000</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>8.250000</td>
      <td>7.500000</td>
      <td>172.500000</td>
    </tr>
    <tr>
      <th>max</th>
      <td>10.500000</td>
      <td>9.200000</td>
      <td>356.000000</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">knn</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span> <span class="o">=</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">knn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">y_train</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>

<span class="c1"># Plotting decision regions</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>

<span class="c1"># Decision region for feature 1 = height = value</span>
<span class="n">value</span><span class="o">=</span><span class="mf">7.6</span>
<span class="c1"># Plot training sample with feature 1 = height = value +/- width</span>
<span class="n">width</span><span class="o">=</span><span class="mf">2.6</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plot_decision_regions</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">y_train</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">clf</span><span class="o">=</span><span class="n">knn</span><span class="p">,</span>
              <span class="n">feature_index</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">],</span>               <span class="c1">#these one will be plotted  </span>
              <span class="n">filler_feature_values</span><span class="o">=</span><span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="n">value</span><span class="p">},</span>  <span class="c1">#these will be ignored</span>
              <span class="n">filler_feature_ranges</span><span class="o">=</span><span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="n">width</span><span class="p">})</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Feature 1 (width)&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Feature 2 (mass)&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Feature 3 (height) = </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">value</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/Users/Kaemyuijang/opt/anaconda3/lib/python3.7/site-packages/mlxtend/plotting/decision_regions.py:279: UserWarning: You passed a edgecolor/edgecolors (&#39;black&#39;) for an unfilled marker (&#39;x&#39;).  Matplotlib is ignoring the edgecolor in favor of the facecolor.  This behavior may change in the future.
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0.5, 1.0, &#39;Feature 3 (height) = 7.6&#39;)
</pre></div>
</div>
<img alt="_images/Chapter7_Introduction_Machine_Learning_90_2.png" src="_images/Chapter7_Introduction_Machine_Learning_90_2.png" />
</div>
</div>
</section>
<section id="width-height-visualization">
<h4><span class="section-number">9.4.4.16. </span>Width-height visualization<a class="headerlink" href="#width-height-visualization" title="Permalink to this headline">¶</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#### Applying KNN classfication with only two features</span>

<span class="n">X_train</span><span class="p">[[</span><span class="s1">&#39;width&#39;</span><span class="p">,</span><span class="s1">&#39;height&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>
<span class="n">y_train</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>

<span class="n">knn</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span> <span class="o">=</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">knn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">[[</span><span class="s1">&#39;width&#39;</span><span class="p">,</span><span class="s1">&#39;height&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">(),</span> <span class="n">y_train</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">())</span>

<span class="c1"># Plotting decision regions</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>

<span class="c1"># Decision region for feature 3 = mass = value</span>
<span class="n">value</span><span class="o">=</span><span class="mi">160</span>
<span class="c1"># Plot training sample with feature 3 = mass = value +/- width</span>
<span class="n">width</span><span class="o">=</span><span class="mi">50</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plot_decision_regions</span><span class="p">(</span><span class="n">X_train</span><span class="p">[[</span><span class="s1">&#39;width&#39;</span><span class="p">,</span><span class="s1">&#39;height&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">(),</span> <span class="n">y_train</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">(),</span> <span class="n">clf</span><span class="o">=</span><span class="n">knn</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Feature 1 (width)&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Feature 2 (height)&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Feature 3 (mass) = </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">value</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/Users/Kaemyuijang/opt/anaconda3/lib/python3.7/site-packages/mlxtend/plotting/decision_regions.py:279: UserWarning: You passed a edgecolor/edgecolors (&#39;black&#39;) for an unfilled marker (&#39;x&#39;).  Matplotlib is ignoring the edgecolor in favor of the facecolor.  This behavior may change in the future.
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0.5, 1.0, &#39;Feature 3 (mass) = 160&#39;)
</pre></div>
</div>
<img alt="_images/Chapter7_Introduction_Machine_Learning_93_2.png" src="_images/Chapter7_Introduction_Machine_Learning_93_2.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">knn</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span> <span class="o">=</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">knn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">y_train</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>

<span class="c1"># Plotting decision regions</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>

<span class="c1"># Decision region for feature 3 = mass = value</span>
<span class="n">value</span><span class="o">=</span><span class="mi">160</span>
<span class="c1"># Plot training sample with feature 3 = mass = value +/- width</span>
<span class="n">width</span><span class="o">=</span><span class="mi">100</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plot_decision_regions</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">y_train</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">clf</span><span class="o">=</span><span class="n">knn</span><span class="p">,</span>
              <span class="n">feature_index</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span>               <span class="c1">#these one will be plotted  </span>
              <span class="n">filler_feature_values</span><span class="o">=</span><span class="p">{</span><span class="mi">2</span><span class="p">:</span> <span class="n">value</span><span class="p">},</span>  <span class="c1">#these will be ignored</span>
              <span class="n">filler_feature_ranges</span><span class="o">=</span><span class="p">{</span><span class="mi">2</span><span class="p">:</span> <span class="n">width</span><span class="p">})</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Feature 1 (width)&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Feature 2 (height)&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Feature 3 (mass) = </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">value</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/Users/Kaemyuijang/opt/anaconda3/lib/python3.7/site-packages/mlxtend/plotting/decision_regions.py:279: UserWarning: You passed a edgecolor/edgecolors (&#39;black&#39;) for an unfilled marker (&#39;x&#39;).  Matplotlib is ignoring the edgecolor in favor of the facecolor.  This behavior may change in the future.
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0.5, 1.0, &#39;Feature 3 (mass) = 160&#39;)
</pre></div>
</div>
<img alt="_images/Chapter7_Introduction_Machine_Learning_94_2.png" src="_images/Chapter7_Introduction_Machine_Learning_94_2.png" />
</div>
</div>
</section>
<section id="visualize-from-scratch-the-decision-regions-of-a-classifier">
<h4><span class="section-number">9.4.4.17. </span>Visualize (from scratch) the decision regions of a classifier<a class="headerlink" href="#visualize-from-scratch-the-decision-regions-of-a-classifier" title="Permalink to this headline">¶</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">matplotlib.colors</span> <span class="kn">import</span> <span class="n">ListedColormap</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">neighbors</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#X = df[[&#39;height&#39;, &#39;width&#39;]].to_numpy()</span>
<span class="c1">#y = df[&#39;fruit_label&#39;].to_numpy()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># The code below has been modified based on https://scikit-learn.org/stable/auto_examples/neighbors/plot_classification.html</span>

<span class="c1"># we only take the first two features. We could avoid this ugly</span>
<span class="c1"># slicing by using a two-dim dataset</span>
<span class="c1">#X = iris.data[:, :2]</span>
<span class="c1">#y = iris.target</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="p">[[</span><span class="s1">&#39;height&#39;</span><span class="p">,</span> <span class="s1">&#39;width&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;fruit_label&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>

<span class="n">n_neighbors</span> <span class="o">=</span> <span class="mi">5</span>

<span class="n">h</span> <span class="o">=</span> <span class="mf">0.02</span>  <span class="c1"># step size in the mesh</span>

<span class="c1"># Create color maps</span>
<span class="n">cmap_light</span> <span class="o">=</span> <span class="n">ListedColormap</span><span class="p">([</span><span class="s2">&quot;orange&quot;</span><span class="p">,</span> <span class="s2">&quot;cyan&quot;</span><span class="p">,</span> <span class="s2">&quot;cornflowerblue&quot;</span><span class="p">,</span><span class="s2">&quot;green&quot;</span><span class="p">])</span>
<span class="n">cmap_bold</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;darkorange&quot;</span><span class="p">,</span> <span class="s2">&quot;c&quot;</span><span class="p">,</span> <span class="s2">&quot;darkblue&quot;</span><span class="p">,</span><span class="s2">&quot;darkgreen&quot;</span><span class="p">]</span>

<span class="k">for</span> <span class="n">weights</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;uniform&quot;</span><span class="p">,</span> <span class="s2">&quot;distance&quot;</span><span class="p">]:</span>
    <span class="c1"># we create an instance of Neighbours Classifier and fit the data.</span>
    <span class="n">clf</span> <span class="o">=</span> <span class="n">neighbors</span><span class="o">.</span><span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="n">weights</span><span class="p">)</span>
    <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

    <span class="c1"># Plot the decision boundary. For that, we will assign a color to each</span>
    <span class="c1"># point in the mesh [x_min, x_max]x[y_min, y_max].</span>
    <span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="n">y_min</span><span class="p">,</span> <span class="n">y_max</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="n">xx</span><span class="p">,</span> <span class="n">yy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span><span class="p">,</span> <span class="n">h</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">y_min</span><span class="p">,</span> <span class="n">y_max</span><span class="p">,</span> <span class="n">h</span><span class="p">))</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">xx</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">yy</span><span class="o">.</span><span class="n">ravel</span><span class="p">()])</span>

    <span class="c1"># Put the result into a color plot</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">Z</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">xx</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">contourf</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">yy</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap_light</span><span class="p">)</span>

    <span class="c1"># Plot also the training points</span>
    <span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span>
        <span class="n">x</span><span class="o">=</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span>
        <span class="n">y</span><span class="o">=</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span>
        <span class="n">hue</span><span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">fruit_name</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">(),</span>
        <span class="n">palette</span><span class="o">=</span><span class="n">cmap_bold</span><span class="p">,</span>
        <span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
        <span class="n">edgecolor</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="n">xx</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">xx</span><span class="o">.</span><span class="n">max</span><span class="p">())</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="n">yy</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">yy</span><span class="o">.</span><span class="n">max</span><span class="p">())</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span>
        <span class="s2">&quot;3-Class classification (k = </span><span class="si">%i</span><span class="s2">, weights = &#39;</span><span class="si">%s</span><span class="s2">&#39;)&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">n_neighbors</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;height&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;width&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/Chapter7_Introduction_Machine_Learning_98_0.png" src="_images/Chapter7_Introduction_Machine_Learning_98_0.png" />
<img alt="_images/Chapter7_Introduction_Machine_Learning_98_1.png" src="_images/Chapter7_Introduction_Machine_Learning_98_1.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#https://stackoverflow.com/questions/52952310/plot-decision-regions-with-error-filler-values-must-be-provided-when-x-has-more</span>
<span class="c1"># You can use PCA to reduce your data multi-dimensional data to two dimensional data. Then pass the obtained result in plot_decision_region and there will be no need of filler values.</span>

<span class="kn">from</span> <span class="nn">mlxtend.plotting</span> <span class="kn">import</span> <span class="n">plot_decision_regions</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Decision region of the training set</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="p">[[</span><span class="s1">&#39;height&#39;</span><span class="p">,</span> <span class="s1">&#39;width&#39;</span><span class="p">]]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;fruit_label&#39;</span><span class="p">]</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">clf</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span> <span class="o">=</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>

<span class="n">plot_decision_regions</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">(),</span> <span class="n">y_train</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">(),</span> <span class="n">clf</span><span class="o">=</span><span class="n">clf</span><span class="p">,</span> <span class="n">legend</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/Users/Kaemyuijang/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:446: UserWarning: X does not have valid feature names, but KNeighborsClassifier was fitted with feature names
/Users/Kaemyuijang/opt/anaconda3/lib/python3.7/site-packages/mlxtend/plotting/decision_regions.py:279: UserWarning: You passed a edgecolor/edgecolors (&#39;black&#39;) for an unfilled marker (&#39;x&#39;).  Matplotlib is ignoring the edgecolor in favor of the facecolor.  This behavior may change in the future.
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;AxesSubplot:&gt;
</pre></div>
</div>
<img alt="_images/Chapter7_Introduction_Machine_Learning_100_2.png" src="_images/Chapter7_Introduction_Machine_Learning_100_2.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Decision region of the test set</span>

<span class="n">plot_decision_regions</span><span class="p">(</span><span class="n">X_test</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">(),</span> <span class="n">y_test</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">(),</span> <span class="n">clf</span><span class="o">=</span><span class="n">clf</span><span class="p">,</span> <span class="n">legend</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/Users/Kaemyuijang/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:446: UserWarning: X does not have valid feature names, but KNeighborsClassifier was fitted with feature names
/Users/Kaemyuijang/opt/anaconda3/lib/python3.7/site-packages/mlxtend/plotting/decision_regions.py:279: UserWarning: You passed a edgecolor/edgecolors (&#39;black&#39;) for an unfilled marker (&#39;x&#39;).  Matplotlib is ignoring the edgecolor in favor of the facecolor.  This behavior may change in the future.
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;AxesSubplot:&gt;
</pre></div>
</div>
<img alt="_images/Chapter7_Introduction_Machine_Learning_101_2.png" src="_images/Chapter7_Introduction_Machine_Learning_101_2.png" />
</div>
</div>
</section>
</section>
</section>
<section id="model-evaluation-metrics-in-machine-learning">
<h2><span class="section-number">9.5. </span>Model Evaluation Metrics in Machine Learning<a class="headerlink" href="#model-evaluation-metrics-in-machine-learning" title="Permalink to this headline">¶</a></h2>
<p>Machine learning has become extremely popular in recent years. Machine learning is used to infer new situations from past data, and there are far too many machine learning algorithms to choose from.</p>
<p>Machine learning techniques such as</p>
<ul class="simple">
<li><p>linear regression,</p></li>
<li><p>logistic regression,</p></li>
<li><p>decision tree,</p></li>
<li><p>Naive Bayes, K-Means, and</p></li>
<li><p>Random Forest</p></li>
</ul>
<p>are widely used.</p>
<p>When it comes to predicting data, we <strong>do not use just one algorithm</strong>. Sometimes we use multiple algorithms and then proceed with the one that gives the best data predictions.</p>
<section id="how-can-we-figure-out-which-algorithm-is-the-most-effective">
<h3><span class="section-number">9.5.1. </span>How can we figure out which algorithm is the most effective?<a class="headerlink" href="#how-can-we-figure-out-which-algorithm-is-the-most-effective" title="Permalink to this headline">¶</a></h3>
<p>Model evaluation metrics allow us to evaluate the accuracy of our trained model and track its performance.</p>
<p>Model evaluation metrics, which distinguish adaptive from non-adaptive machine learning models, indicate how effectively the model generalizes to new data.</p>
<p>We could improve the overall predictive power of our model before using it for production on unknown data by using different performance evaluation metrics.</p>
<p>Choosing the right metric is very important when evaluating machine learning models. Machine learning models are evaluated using a variety of metrics in different applications. Let us look at the metrics for evaluating the performance of a machine learning model.</p>
<p>This is a critical phase in any data science project as it aims to estimate the generalization accuracy of a model for future data.</p>
<p><strong>Evaluation Metrics For Regression Models</strong>: image from enjoyalgorithms
<img alt="Evaluation Metrics For Regression Models from enjoyalgorithms" src="https://www.enjoyalgorithms.com/static/evaluation-metrics-regression-models-cover-6422d3e49173675d75d121740c04d450.jpg" /></p>
<p><strong>Evaluation Metrics For Classification Models</strong>: image from enjoyalgorithms
<img alt="Evaluation Metrics For Classification Models from enjoyalgorithms" src="https://www.enjoyalgorithms.com/static/evaluation-metrics-classification-models-cover-4f403c2e47e719b4389b4b2d05d71c34.jpg" /></p>
<section id="regression-related-metrics">
<h4><span class="section-number">9.5.1.1. </span>Regression Related Metrics<a class="headerlink" href="#regression-related-metrics" title="Permalink to this headline">¶</a></h4>
<p>The most common measures for evaluating a regression model (as used in our previous chapter) are:</p>
<ul class="simple">
<li><p><strong>Mean Absolute Error (MAE)</strong>: The average of the difference between the actual and anticipated values is the Mean Absolute Error. It determines how close the predictions are to the actual results. The better the model, the lower the MAE.</p></li>
<li><p><strong>Mean Squared Error (MSE)</strong>: The average of the square of the difference between the actual and predicted values is calculated by MSE.</p></li>
<li><p><strong>R2 score</strong>: The proportion of variance in Y that can be explained by X is called the R2 score.</p></li>
</ul>
</section>
<section id="classification-metrics">
<h4><span class="section-number">9.5.1.2. </span>Classification Metrics<a class="headerlink" href="#classification-metrics" title="Permalink to this headline">¶</a></h4>
<ol class="simple">
<li><p><strong>Confusion Matrix (Accuracy, Sensitivity, and Specificity)</strong></p></li>
</ol>
<p>A confusion matrix contains the results of any binary testing that is commonly used to describe the classification model’s performance.</p>
<p>In a binary classification task, there are only two classes to categorize, preferably a <strong>positive class</strong> and a <strong>negative class</strong>.</p>
<p>Let us take a look at the metrics of the confusion matrix.</p>
<ul class="simple">
<li><p><strong>Accuracy</strong>: indicates the overall accuracy of the model, i.e., the percentage of all samples that were correctly identified by the classifier. Use the following formula to calculate accuracy:
(TP +TN)/(TP +TN+ FP +FN).</p>
<ul>
<li><p>True Positive (TP): This is the number of times the classifier successfully predicted the positive class to be positive.</p></li>
<li><p>True Negative (TN): The number of times the classifier correctly predicts the negative class as negative.</p></li>
<li><p>False Positive (FP): This term refers to the number of times a classifier incorrectly predicts a negative class as positive.</p></li>
<li><p>False Negative (FN): This is the number of times the classifier predicts the positive class as negative.</p></li>
</ul>
</li>
<li><p><strong>The misclassification rate</strong>: tells you what percentage of predictions were incorrect. It is also called classification error. You can calculate it with (FP +FN)/(TP +TN+ FP +FN) or (1-accuracy).</p></li>
<li><p><strong>Sensitivity (or Recall)</strong>: It indicates the proportion of all positive samples that were correctly predicted to be positive by the classifier. It is also referred to as <strong>true positive rate (TPR), sensitivity, or probability of detection</strong>. To calculate recall, use the following formula: TP /(TP +FN).</p></li>
<li><p><strong>Specificity</strong>: it indicates the proportion of all negative samples that are correctly predicted to be negative by the classifier. It is also referred to as the <strong>True Negative Rate (TNR)</strong>. To calculate the specificity, use the following formula: TN /(TN +FP).</p></li>
</ul>
<ol class="simple">
<li><p><strong>Precision</strong>: When there is an <strong>imbalance between classes</strong>, accuracy can become an unreliable metric for measuring our performance. Therefore, we also need to address class-specific performance metrics. Precision is one such metric, defined as <strong>positive predictive values</strong> (Proportion of predictions as positive class were actually positive). To calculate precision, use the following formula: TP/(TP+FP).</p></li>
<li><p><strong>F1-score</strong>: it combines precision and recall in a single measure. Mathematically, it is the harmonic mean of Precision and Recall. It can be calculated as follows:</p></li>
</ol>
<div class="math notranslate nohighlight">
\[{\displaystyle F_{1}={\frac {2}{\mathrm {recall^{-1}} +\mathrm {precision^{-1}} }}=2\cdot {\frac {\mathrm {precision} \cdot \mathrm {recall} }{\mathrm {precision} +\mathrm {recall} }}={\frac {\mathrm {tp} }{\mathrm {tp} +{\frac {1}{2}}(\mathrm {fp} +\mathrm {fn} )}}}.\]</div>
<p>In a perfect world, we would want a model that has a precision of 1 and a recall of 1. This means an F1 score of 1, i.e. 100% accuracy, which is often not the case for a machine learning model. So we should try to achieve a higher precision with a higher recall value.</p>
<p><strong>Confusion Matrix for Binary Classification</strong>: image from <a class="reference external" href="https://towardsdatascience.com/">https://towardsdatascience.com/</a>
<img alt="Confusion Matrix for Binary Classification image from https://towardsdatascience.com/" src="https://miro.medium.com/max/700/1*fxiTNIgOyvAombPJx5KGeA.png" /></p>
<p><strong>Confusion Matrix for Multi-class Classification</strong>: image from <a class="reference external" href="https://towardsdatascience.com/">https://towardsdatascience.com/</a>
<img alt="Confusion Matrix for Multi-class Classification image from https://towardsdatascience.com/" src="https://miro.medium.com/max/700/1*yH2SM0DIUQlEiveK42NnBg.png" /></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Importing a dataset</span>
<span class="n">url</span> <span class="o">=</span> <span class="s1">&#39;https://raw.githubusercontent.com/susanli2016/Machine-Learning-with-Python/master/fruit_data_with_colors.txt&#39;</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_table</span><span class="p">(</span><span class="n">url</span><span class="p">)</span>  

<span class="c1"># Train Test Split</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="p">[[</span><span class="s1">&#39;height&#39;</span><span class="p">,</span> <span class="s1">&#39;width&#39;</span><span class="p">,</span> <span class="s1">&#39;mass&#39;</span><span class="p">]]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;fruit_label&#39;</span><span class="p">]</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Instantiate the estimator</span>
<span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>
<span class="n">knn</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span> <span class="o">=</span> <span class="mi">5</span><span class="p">)</span>

<span class="c1"># Training the classifier by passing in the training set X_train and the labels in y_train</span>
<span class="n">knn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Predicting labels for unknown data</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">knn</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Various attributes of the knn estimator</span>

<span class="nb">print</span><span class="p">(</span><span class="n">knn</span><span class="o">.</span><span class="n">classes_</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">knn</span><span class="o">.</span><span class="n">feature_names_in_</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">knn</span><span class="o">.</span><span class="n">n_features_in_</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">knn</span><span class="o">.</span><span class="n">n_neighbors</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[1 2 3 4]
[&#39;height&#39; &#39;width&#39; &#39;mass&#39;]
3
5
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#importing confusion matrix</span>

<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">confusion_matrix</span><span class="p">,</span> <span class="n">ConfusionMatrixDisplay</span>
<span class="n">confusion</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Confusion Matrix</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">confusion</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Confusion Matrix

[[3 0 0 1]
 [0 1 0 0]
 [3 0 3 2]
 [0 0 1 1]]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Confusion Matrix visualization.</span>

<span class="n">cm</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">knn</span><span class="o">.</span><span class="n">classes_</span><span class="p">)</span>
<span class="n">disp</span> <span class="o">=</span> <span class="n">ConfusionMatrixDisplay</span><span class="p">(</span><span class="n">confusion_matrix</span><span class="o">=</span><span class="n">cm</span><span class="p">,</span><span class="n">display_labels</span><span class="o">=</span><span class="n">knn</span><span class="o">.</span><span class="n">classes_</span><span class="p">)</span>
<span class="n">disp</span><span class="o">.</span><span class="n">plot</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x125e687d0&gt;
</pre></div>
</div>
<img alt="_images/Chapter7_Introduction_Machine_Learning_113_1.png" src="_images/Chapter7_Introduction_Machine_Learning_113_1.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;observed&#39;</span><span class="p">:</span> <span class="n">y_test</span> <span class="p">,</span><span class="s1">&#39;predicted&#39;</span><span class="p">:</span><span class="n">knn</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)})</span><span class="o">.</span><span class="n">set_index</span><span class="p">(</span><span class="n">X_test</span><span class="o">.</span><span class="n">index</span><span class="p">)</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span> <span class="o">=</span> <span class="s1">&#39;observed&#39;</span><span class="p">)</span> 
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>observed</th>
      <th>predicted</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>11</th>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>22</th>
      <td>1</td>
      <td>4</td>
    </tr>
    <tr>
      <th>10</th>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>26</th>
      <td>3</td>
      <td>3</td>
    </tr>
    <tr>
      <th>35</th>
      <td>3</td>
      <td>1</td>
    </tr>
    <tr>
      <th>28</th>
      <td>3</td>
      <td>4</td>
    </tr>
    <tr>
      <th>34</th>
      <td>3</td>
      <td>3</td>
    </tr>
    <tr>
      <th>40</th>
      <td>3</td>
      <td>1</td>
    </tr>
    <tr>
      <th>30</th>
      <td>3</td>
      <td>3</td>
    </tr>
    <tr>
      <th>41</th>
      <td>3</td>
      <td>1</td>
    </tr>
    <tr>
      <th>33</th>
      <td>3</td>
      <td>4</td>
    </tr>
    <tr>
      <th>43</th>
      <td>4</td>
      <td>4</td>
    </tr>
    <tr>
      <th>46</th>
      <td>4</td>
      <td>3</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Unlike the binary classification, there are no positive or negative classes here.</p>
<p>At first glance, it might be a little difficult to find TP, TN, FP, and FN since there are no positive or negative classes, but it’s actually pretty simple.</p>
<p>What we need to do here is find TP, TN, FP and FN for each and every class. For example, let us take the <strong>Apple class</strong>. Let us look at what values the metrics have in the confusion matrix.
(DO NOT FORGET TO TRANSPOSE)</p>
<ul class="simple">
<li><p>TP = 3</p></li>
<li><p>TN = (1 + 3 + 2 + 1 + 1) = 8 (the sum of the numbers in rows 2-4 and columns 2-4)</p></li>
<li><p>FP = (0 + 3 + 0) = 3</p></li>
<li><p>FN = (0 + 0 + 1) = 1</p></li>
</ul>
<p>Now that we have all the necessary metrics for the Apple class from the confusion matrix, we can calculate the performance metrics for the Apple class. For example, the class Apple has</p>
<ul class="simple">
<li><p>Precision = 3/(3+3) = 0.5</p></li>
<li><p>Recall = 3/(3+1) = 0.75</p></li>
<li><p>F1-score = 0.60</p></li>
</ul>
<p>In a similar way, we can calculate the measures for the other classes. Here is a table showing the values of each measure for each class.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">classification_report</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Classification Report</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">target_names</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Class 1&#39;</span><span class="p">,</span> <span class="s1">&#39;Class 2&#39;</span><span class="p">,</span> <span class="s1">&#39;Class 3&#39;</span><span class="p">,</span> <span class="s1">&#39;Class 4&#39;</span><span class="p">]))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Classification Report

              precision    recall  f1-score   support

     Class 1       0.50      0.75      0.60         4
     Class 2       1.00      1.00      1.00         1
     Class 3       0.75      0.38      0.50         8
     Class 4       0.25      0.50      0.33         2

    accuracy                           0.53        15
   macro avg       0.62      0.66      0.61        15
weighted avg       0.63      0.53      0.54        15
</pre></div>
</div>
</div>
</div>
<p>Now we can do more with these measures. We can combine the F1 score of each class to get a single measure for the entire model. There are several ways to do this, which we will now look at.</p>
<ul class="simple">
<li><p><strong>Macro F1</strong>
This is the macro-averaged F1 score. It calculates the metrics for each class separately and then takes the unweighted average of the measures. As we saw in the figure “Precision, recall and F1 score for each class”,</p></li>
<li><p><strong>Weighted F1</strong>
The final value is the weighted mean F1 score. Unlike Macro F1, this uses a weighted mean of the measures. The weights for each class are the total number of samples in that class. Since we had 4 apples, 1 mandarin, 8 oranges, and 3 lemons,</p></li>
</ul>
<p>We obtain a classficiation rate of 53.3%, considered as good accuracy.</p>
<p>Can we further improve the accuracy of the KNN algorithm?</p>
<p>In our example, we have created an instance (‘knn’) of the class ‘KNeighborsClassifer,’ which means we have constructed an object called ‘knn’ that knows how to perform KNN classification once the data is provided.</p>
<p>The tuning parameter/hyper parameter (K) is the parameter <strong>n_neighbors</strong>. All other parameters are set to default values.</p>
<p><strong>Exercises</strong></p>
<ol class="simple">
<li><p>Fit the model and test it for different values for K (from 1 to 5) using a for loop and record the KNN’s testing accuracy of the KNN in a variable.</p></li>
<li><p>Plot the relationship between the values of K and the corresponding testing accuracy.</p></li>
<li><p>Select the optimal value of K that gives the highest testing accuray.</p></li>
<li><p>Compare the results between the optimal value of K and K = 5.</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Importing a dataset</span>
<span class="n">url</span> <span class="o">=</span> <span class="s1">&#39;https://raw.githubusercontent.com/susanli2016/Machine-Learning-with-Python/master/fruit_data_with_colors.txt&#39;</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_table</span><span class="p">(</span><span class="n">url</span><span class="p">)</span>  

<span class="c1"># Train Test Split</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="p">[[</span><span class="s1">&#39;height&#39;</span><span class="p">,</span> <span class="s1">&#39;width&#39;</span><span class="p">,</span> <span class="s1">&#39;mass&#39;</span><span class="p">]]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;fruit_label&#39;</span><span class="p">]</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Instantiate the estimator</span>
<span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>
<span class="n">knn</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">()</span>

<span class="n">k_range</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">15</span><span class="p">)</span>

<span class="n">train_accuracy</span> <span class="o">=</span> <span class="p">{}</span>
<span class="n">train_accuracy_list</span> <span class="o">=</span> <span class="p">[]</span>

<span class="n">test_accuracy</span> <span class="o">=</span> <span class="p">{}</span>
<span class="n">test_accuracy_list</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">k_range</span><span class="p">:</span>
    <span class="n">knn</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span> <span class="o">=</span> <span class="n">k</span><span class="p">)</span>
    
    <span class="c1"># Training the classifier by passing in the training set X_train and the labels in y_train</span>
    <span class="n">knn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
    
    <span class="c1"># Compute accuracy on the training set</span>
    <span class="n">train_accuracy</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">knn</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">train_accuracy_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">knn</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">))</span>
    
    
    <span class="n">test_accuracy</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">knn</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span><span class="n">y_test</span><span class="p">)</span>
    <span class="n">test_accuracy_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">knn</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span><span class="n">y_test</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df_output</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;k&#39;</span><span class="p">:</span><span class="n">k_range</span><span class="p">,</span>
                          <span class="s1">&#39;train_accuracy&#39;</span><span class="p">:</span><span class="n">train_accuracy_list</span><span class="p">,</span>
                          <span class="s1">&#39;test_accuracy&#39;</span><span class="p">:</span><span class="n">test_accuracy_list</span>
                         <span class="p">})</span>

<span class="p">(</span>
    <span class="n">ggplot</span><span class="p">(</span><span class="n">df_output</span><span class="p">)</span> 
    <span class="o">+</span> <span class="n">geom_line</span><span class="p">(</span><span class="n">aes</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="s1">&#39;k&#39;</span><span class="p">,</span>  <span class="n">y</span> <span class="o">=</span> <span class="s1">&#39;train_accuracy&#39;</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;&quot;training accuracy&quot;&#39;</span><span class="p">))</span>
    <span class="o">+</span> <span class="n">geom_line</span><span class="p">(</span><span class="n">aes</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="s1">&#39;k&#39;</span><span class="p">,</span>  <span class="n">y</span> <span class="o">=</span> <span class="s1">&#39;test_accuracy&#39;</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;&quot;testing accuracy&quot;&#39;</span><span class="p">))</span>
    <span class="o">+</span> <span class="n">labs</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s1">&#39;k (n_neighbors)&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s1">&#39;overall accuracy&#39;</span><span class="p">,</span> <span class="n">title</span> <span class="o">=</span> <span class="s1">&#39;Model Accuracy&#39;</span><span class="p">)</span>
    <span class="o">+</span> <span class="n">scale_color_manual</span><span class="p">(</span><span class="n">values</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;blue&quot;</span><span class="p">,</span> <span class="s2">&quot;green&quot;</span><span class="p">],</span> <span class="c1"># Colors</span>
        <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot; &quot;</span><span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/Chapter7_Introduction_Machine_Learning_121_0.png" src="_images/Chapter7_Introduction_Machine_Learning_121_0.png" />
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;ggplot: (306375789)&gt;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df_output</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span> <span class="o">=</span> <span class="s1">&#39;test_accuracy&#39;</span><span class="p">,</span>  <span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>k</th>
      <th>train_accuracy</th>
      <th>test_accuracy</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>1.000000</td>
      <td>0.600000</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>0.886364</td>
      <td>0.533333</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>0.818182</td>
      <td>0.533333</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>0.795455</td>
      <td>0.533333</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>0.795455</td>
      <td>0.533333</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">row_max</span> <span class="o">=</span> <span class="n">df_output</span><span class="o">.</span><span class="n">test_accuracy</span><span class="o">.</span><span class="n">idxmax</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Best accuracy was </span><span class="si">{</span><span class="n">df_output</span><span class="o">.</span><span class="n">test_accuracy</span><span class="p">[</span><span class="n">row_max</span><span class="p">]</span><span class="si">}</span><span class="s1">, which corresponds to a value of K=</span><span class="si">{</span><span class="n">df_output</span><span class="o">.</span><span class="n">k</span><span class="p">[</span><span class="n">row_max</span><span class="p">]</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Best accuracy was 0.6, which corresponds to a value of K=1
</pre></div>
</div>
</div>
</div>
<p>The overall accuracy of 0.6 shows that the model does not perform well in the predictions for the test data set.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#y_test.shape</span>
<span class="c1">#np.sqrt(15)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sns</span><span class="o">.</span><span class="n">pairplot</span><span class="p">(</span><span class="n">df</span><span class="p">[[</span><span class="s1">&#39;fruit_name&#39;</span><span class="p">,</span><span class="s1">&#39;mass&#39;</span><span class="p">,</span><span class="s1">&#39;width&#39;</span><span class="p">,</span><span class="s1">&#39;height&#39;</span><span class="p">,</span><span class="s1">&#39;color_score&#39;</span><span class="p">]],</span><span class="n">hue</span><span class="o">=</span><span class="s1">&#39;fruit_name&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;seaborn.axisgrid.PairGrid at 0x124397c50&gt;
</pre></div>
</div>
<img alt="_images/Chapter7_Introduction_Machine_Learning_126_1.png" src="_images/Chapter7_Introduction_Machine_Learning_126_1.png" />
</div>
</div>
</section>
<section id="knn-model-with-color-score-and-other-feature-s">
<h4><span class="section-number">9.5.1.3. </span>KNN model with <strong>color_score</strong> and other feature(s)<a class="headerlink" href="#knn-model-with-color-score-and-other-feature-s" title="Permalink to this headline">¶</a></h4>
<p>We will see if we can improve the accuracy of our model by including the feature <strong>color_score</strong>.</p>
<p>As we can see from the pair plots and correlation matrix below,</p>
<ul class="simple">
<li><p>there is a clear nonlinear separation of the 4 fruit types when we consider <strong>color_score</strong>.</p></li>
<li><p>Also, we see that both mass and width have a positive correlation with height.</p></li>
</ul>
<p>Therefore, we may omit the features <strong>weight</strong> and <strong>mass</strong> and train the KNN model with the two features <strong>height</strong> and <strong>color_score</strong> instead.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sns</span><span class="o">.</span><span class="n">pairplot</span><span class="p">(</span><span class="n">df</span><span class="p">[[</span><span class="s1">&#39;fruit_name&#39;</span><span class="p">,</span><span class="s1">&#39;mass&#39;</span><span class="p">,</span><span class="s1">&#39;width&#39;</span><span class="p">,</span><span class="s1">&#39;height&#39;</span><span class="p">,</span><span class="s1">&#39;color_score&#39;</span><span class="p">]],</span><span class="n">hue</span><span class="o">=</span><span class="s1">&#39;fruit_name&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;seaborn.axisgrid.PairGrid at 0x124c38810&gt;
</pre></div>
</div>
<img alt="_images/Chapter7_Introduction_Machine_Learning_130_1.png" src="_images/Chapter7_Introduction_Machine_Learning_130_1.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">hm</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">corr</span><span class="p">(),</span> <span class="n">annot</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
<span class="n">hm</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">title</span> <span class="o">=</span> <span class="s2">&quot;Correlation matrix of insurance data</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/Chapter7_Introduction_Machine_Learning_131_0.png" src="_images/Chapter7_Introduction_Machine_Learning_131_0.png" />
</div>
</div>
<p>Let us make some visualizations with pair plots of the two features <strong>height</strong> and <strong>color_score</strong> instead.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sns</span><span class="o">.</span><span class="n">pairplot</span><span class="p">(</span><span class="n">df</span><span class="p">[[</span><span class="s1">&#39;fruit_name&#39;</span><span class="p">,</span><span class="s1">&#39;color_score&#39;</span><span class="p">,</span><span class="s1">&#39;height&#39;</span><span class="p">]],</span><span class="n">hue</span><span class="o">=</span><span class="s1">&#39;fruit_name&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;seaborn.axisgrid.PairGrid at 0x126ef57d0&gt;
</pre></div>
</div>
<img alt="_images/Chapter7_Introduction_Machine_Learning_133_1.png" src="_images/Chapter7_Introduction_Machine_Learning_133_1.png" />
</div>
</div>
<p>We will repeat the same process as before:</p>
<ul class="simple">
<li><p>Spliting the data into training and test sets,</p></li>
<li><p>Fitting the model on the training dataset,</p></li>
<li><p>Making predictions on new dataset (test set), and</p></li>
<li><p>Evaluating the predictive performances on the test set</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Importing a dataset</span>
<span class="n">url</span> <span class="o">=</span> <span class="s1">&#39;https://raw.githubusercontent.com/susanli2016/Machine-Learning-with-Python/master/fruit_data_with_colors.txt&#39;</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_table</span><span class="p">(</span><span class="n">url</span><span class="p">)</span>  

<span class="c1"># Train Test Split</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="p">[[</span><span class="s1">&#39;height&#39;</span><span class="p">,</span> <span class="s1">&#39;color_score&#39;</span><span class="p">]]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;fruit_label&#39;</span><span class="p">]</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Instantiate the estimator</span>
<span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>
<span class="c1">#knn = KNeighborsClassifier()</span>

<span class="n">k_range</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">16</span><span class="p">)</span>

<span class="n">train_accuracy</span> <span class="o">=</span> <span class="p">{}</span>
<span class="n">train_accuracy_list</span> <span class="o">=</span> <span class="p">[]</span>

<span class="n">test_accuracy</span> <span class="o">=</span> <span class="p">{}</span>
<span class="n">test_accuracy_list</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># misclassfication error</span>
<span class="n">train_error_list</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">test_error_list</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">k_range</span><span class="p">:</span>
    <span class="n">knn</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span> <span class="o">=</span> <span class="n">k</span><span class="p">)</span>
    
    <span class="c1"># Training the classifier by passing in the training set X_train and the labels in y_train</span>
    <span class="n">knn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
    
    <span class="c1"># Compute accuracy on the training set</span>
    <span class="n">train_accuracy</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">knn</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">train_accuracy_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">knn</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">))</span>
    <span class="n">train_error_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">knn</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">))</span>
    
    <span class="n">test_accuracy</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">knn</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span><span class="n">y_test</span><span class="p">)</span>
    <span class="n">test_accuracy_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">knn</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span><span class="n">y_test</span><span class="p">))</span>
    <span class="n">test_error_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">knn</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span><span class="n">y_test</span><span class="p">))</span>
    
<span class="n">df_output</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;k&#39;</span><span class="p">:</span><span class="n">k_range</span><span class="p">,</span>
                          <span class="s1">&#39;train_accuracy&#39;</span><span class="p">:</span><span class="n">train_accuracy_list</span><span class="p">,</span>
                          <span class="s1">&#39;test_accuracy&#39;</span><span class="p">:</span><span class="n">test_accuracy_list</span><span class="p">,</span>
                          <span class="s1">&#39;train_error&#39;</span><span class="p">:</span><span class="n">train_error_list</span><span class="p">,</span>
                          <span class="s1">&#39;test_error&#39;</span><span class="p">:</span><span class="n">test_error_list</span>
                         <span class="p">})</span>    
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Accuracy over the number of K neighbors</span>

<span class="p">(</span>
    <span class="n">ggplot</span><span class="p">(</span><span class="n">df_output</span><span class="p">)</span> 
    <span class="o">+</span> <span class="n">geom_line</span><span class="p">(</span><span class="n">aes</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="s1">&#39;k&#39;</span><span class="p">,</span>  <span class="n">y</span> <span class="o">=</span> <span class="s1">&#39;train_accuracy&#39;</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;&quot;training accuracy&quot;&#39;</span><span class="p">))</span>
    <span class="o">+</span> <span class="n">geom_line</span><span class="p">(</span><span class="n">aes</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="s1">&#39;k&#39;</span><span class="p">,</span>  <span class="n">y</span> <span class="o">=</span> <span class="s1">&#39;test_accuracy&#39;</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;&quot;testing accuracy&quot;&#39;</span><span class="p">))</span>
    <span class="o">+</span> <span class="n">labs</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s1">&#39;k (n_neighbors)&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s1">&#39;overall accuracy&#39;</span><span class="p">)</span>
    <span class="o">+</span> <span class="n">scale_color_manual</span><span class="p">(</span><span class="n">values</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;blue&quot;</span><span class="p">,</span> <span class="s2">&quot;green&quot;</span><span class="p">],</span> <span class="c1"># Colors</span>
        <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot; &quot;</span><span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/Chapter7_Introduction_Machine_Learning_136_0.png" src="_images/Chapter7_Introduction_Machine_Learning_136_0.png" />
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;ggplot: (309539749)&gt;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">row_max</span> <span class="o">=</span> <span class="n">df_output</span><span class="o">.</span><span class="n">test_accuracy</span><span class="o">.</span><span class="n">idxmax</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Best accuracy was </span><span class="si">{</span><span class="n">df_output</span><span class="o">.</span><span class="n">test_accuracy</span><span class="p">[</span><span class="n">row_max</span><span class="p">]</span><span class="si">}</span><span class="s1">, which corresponds to a value of K=</span><span class="si">{</span><span class="n">df_output</span><span class="o">.</span><span class="n">k</span><span class="p">[</span><span class="n">row_max</span><span class="p">]</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Best accuracy was 0.7333333333333333, which corresponds to a value of K=4
</pre></div>
</div>
</div>
</div>
<p>From the results above, we see that the performance of KNN model increase to values around 73.33% in accuracy.</p>
</section>
<section id="overfitting-and-underfitting-the-misclassification-rate-vs-k">
<h4><span class="section-number">9.5.1.4. </span>Overfitting and Underfitting (The Misclassification Rate vs K)<a class="headerlink" href="#overfitting-and-underfitting-the-misclassification-rate-vs-k" title="Permalink to this headline">¶</a></h4>
<p>When implementing KNN (or other machine learning algorithms), one of the most important questions to consider is related to the choice of the number of neighbors (k) to use.</p>
<p>However, you should be aware of two issues that may arise as a result of the number of neighbors (k) you choose: <strong>underfitting</strong> and <strong>overfitting</strong>.</p>
<ol class="simple">
<li><p><strong>Underfitting</strong></p></li>
</ol>
<p>Underfitting occurs when there are</p>
<ul class="simple">
<li><p>too few predictors or</p></li>
<li><p>a model that is too simplistic to accurately capture the data’s relationships/patterns (large K in KNN).</p></li>
</ul>
<p>As a result, a biased model arises, one that <strong>performs poorly on both the data we used to train it and new data (low accuracy or high misclassification error)</strong>.</p>
<ol class="simple">
<li><p><strong>Overfitting</strong>
Overfitting is the opposite of underfitting, and it occurs when</p></li>
</ol>
<ul class="simple">
<li><p>we use too many predictors or</p></li>
<li><p>a model that is too complex (small K in KNN),</p></li>
</ul>
<p>resulting in a model that models not just the relationships/patterns in our data, but also the noise.</p>
<p>Noise in a dataset is variance that is not consistently related to the variables we have observed, but is instead caused by inherent variability and/or measurement error.</p>
<p>Because the pattern of noise is so unique to each dataset, if we try to represent it, our model may perform <strong>very well on the data we used to train it but produce poor prediction results on new datasets</strong>.</p>
<p>The following Python illustrate the concepts of underfitting and overfitting. It plots the misclassification rate (1 - accuracy) over the number of K neighbors.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Error over the number of K neighbors</span>

<span class="p">(</span>
    <span class="n">ggplot</span><span class="p">(</span><span class="n">df_output</span><span class="p">)</span> 
    <span class="o">+</span> <span class="n">geom_line</span><span class="p">(</span><span class="n">aes</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="s1">&#39;k&#39;</span><span class="p">,</span>  <span class="n">y</span> <span class="o">=</span> <span class="s1">&#39;train_error&#39;</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;&quot;training error&quot;&#39;</span><span class="p">))</span>
    <span class="o">+</span> <span class="n">geom_line</span><span class="p">(</span><span class="n">aes</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="s1">&#39;k&#39;</span><span class="p">,</span>  <span class="n">y</span> <span class="o">=</span> <span class="s1">&#39;test_error&#39;</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;&quot;testing error&quot;&#39;</span><span class="p">))</span>
    <span class="o">+</span> <span class="n">labs</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s1">&#39;Model complexity: k (n_neighbors)&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s1">&#39;Error&#39;</span><span class="p">)</span>
    <span class="o">+</span> <span class="n">scale_color_manual</span><span class="p">(</span><span class="n">values</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;blue&quot;</span><span class="p">,</span> <span class="s2">&quot;green&quot;</span><span class="p">],</span> <span class="c1"># Colors</span>
        <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot; &quot;</span><span class="p">)</span>
    <span class="o">+</span> <span class="n">scale_x_continuous</span><span class="p">(</span><span class="n">trans</span> <span class="o">=</span> <span class="s2">&quot;reverse&quot;</span><span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/Chapter7_Introduction_Machine_Learning_141_0.png" src="_images/Chapter7_Introduction_Machine_Learning_141_0.png" />
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;ggplot: (309779273)&gt;
</pre></div>
</div>
</div>
</div>
<section id="overfitting-and-underfitting">
<h5><span class="section-number">9.5.1.4.1. </span>Overfitting and Underfitting<a class="headerlink" href="#overfitting-and-underfitting" title="Permalink to this headline">¶</a></h5>
<p><img alt="Overfitting and Underfitting : image from vitalflux" src="https://vitalflux.com/wp-content/uploads/2020/12/overfitting-and-underfitting-wrt-model-error-vs-complexity-300x173.png" /></p>
<p>In the figure above, we see that increasing K increases our error rate in the training set. The predictions begin to become biased (i.e., “over smoothing”) by creating prediction values that approach the mean of the observed data set</p>
<p>In contrast, we get minimal error in the training set if we use K = 1, i.e. the model is just memorising the data.</p>
<p>What we are interested in, however, is the generalization error (test error), i.e., the expected value of the misclassification rate when averaged over new data</p>
<p>This value can be approximated by computing the misclassification rate on a large independent test set that was not used in training the model.</p>
<p>We plot the test error against K in green (upper curve). Now we see a <strong>U-shaped curve</strong>: for complex models (small K), the method overfits, and for simple models (big K), the method underfits.</p>
</section>
</section>
<section id="visualization-of-decision-regions-knn-model-with-two-features-height-and-color-score">
<h4><span class="section-number">9.5.1.5. </span>Visualization of decision regions (KNN model with two features, <strong>height</strong> and <strong>color_score</strong>)<a class="headerlink" href="#visualization-of-decision-regions-knn-model-with-two-features-height-and-color-score" title="Permalink to this headline">¶</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">##### Convert pandas DataFrame to Numpy before applying classification</span>

<span class="n">X_train_np</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>
<span class="n">y_train_np</span> <span class="o">=</span> <span class="n">y_train</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>

<span class="n">X_test_np</span> <span class="o">=</span> <span class="n">X_test</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>
<span class="n">y_test_np</span> <span class="o">=</span> <span class="n">y_test</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>

<span class="n">clf</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span> <span class="o">=</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_np</span><span class="p">,</span> <span class="n">y_train_np</span><span class="p">)</span>

<span class="c1"># Plotting decision regions</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>

<span class="c1"># Decision region for feature 3 = mass = value</span>
<span class="c1">#value=160</span>
<span class="c1"># Plot training sample with feature = mass = value +/- width</span>
<span class="c1">#width=20</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plot_decision_regions</span><span class="p">(</span><span class="n">X_test_np</span><span class="p">,</span> <span class="n">y_test_np</span><span class="p">,</span> <span class="n">clf</span><span class="o">=</span><span class="n">clf</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Feature 1 (height)&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Feature 2 (color_scale)&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Classification problem&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/Users/Kaemyuijang/opt/anaconda3/lib/python3.7/site-packages/mlxtend/plotting/decision_regions.py:279: UserWarning: You passed a edgecolor/edgecolors (&#39;black&#39;) for an unfilled marker (&#39;x&#39;).  Matplotlib is ignoring the edgecolor in favor of the facecolor.  This behavior may change in the future.
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0.5, 1.0, &#39;Classification problem&#39;)
</pre></div>
</div>
<img alt="_images/Chapter7_Introduction_Machine_Learning_145_2.png" src="_images/Chapter7_Introduction_Machine_Learning_145_2.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#clf.predict([[ 8.5 ,  0],</span>
<span class="c1">#       [9.2 ,  0]])</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
</section>
<section id="scaling-features-in-knn">
<h2><span class="section-number">9.6. </span>Scaling Features in KNN<a class="headerlink" href="#scaling-features-in-knn" title="Permalink to this headline">¶</a></h2>
<p>KNN is a distance-based algorithm, where KNN classifies data based on proximity to K neighbors. Then, we often find that the features of the data we are using are not on the same scale/unit. An example of this is the characteristics of weight and height. Obviously these two features have different units, the feature weight is in kilograms and height is in centimeters.</p>
<p>Because this unit difference causes Distance-Based algorithms including KNN to perform poorly, rescaling features with different units to the same scale/units is required to <strong>improve performance of K-Nearest Neighbors</strong>.</p>
<p>Another reason for using feature scaling is that some algorithms, such as gradient descent in neural networks, converge faster with it than without it.</p>
<section id="common-techniques-of-feature-scaling">
<h3><span class="section-number">9.6.1. </span>Common techniques of feature scaling<a class="headerlink" href="#common-techniques-of-feature-scaling" title="Permalink to this headline">¶</a></h3>
<p>There are a variety of methods for rescaling features including</p>
<ul class="simple">
<li><p><strong>Min-Max Scaling</strong></p></li>
</ul>
<p>The simplest method, also known as <strong>min-max scaling</strong> or <strong>min-max normalization</strong>, consists of rescaling the range of features to scale the range in [0, 1] or [1, 1]. The goal range is determined by the data’s type. The following is the general formula for a min-max of [0, 1]:</p>
<div class="math notranslate nohighlight">
\[x'={\frac  {x-{\text{min}}(x)}{{\text{max}}(x)-{\text{min}}(x)}}\]</div>
<p>where <span class="math notranslate nohighlight">\(\displaystyle x\)</span> is the original value and <span class="math notranslate nohighlight">\(\displaystyle x'\)</span> is the normalized value.</p>
<ul class="simple">
<li><p><strong>Standard Scaling</strong></p></li>
</ul>
<p><strong>Feature standardization</strong>, also known as Z-score Normalization, ensures that the values of each feature in the data have a mean of zero (when subtracting the mean in the numerator) and a unit variance.</p>
<div class="math notranslate nohighlight">
\[x' = \frac{x - \bar{x}}{\sigma}\]</div>
<p>where <span class="math notranslate nohighlight">\({\displaystyle x}\)</span> is the original feature vector, <span class="math notranslate nohighlight">\({\displaystyle {\bar {x}}={\text{average}}(x)}\)</span> is the mean of that feature vector, and <span class="math notranslate nohighlight">\({\displaystyle \sigma }\)</span>  is its standard deviation.</p>
<p>This method is commonly utilized in various machine learning methods for normalization (e.g., support vector machines, logistic regression, and artificial neural networks)</p>
<ul class="simple">
<li><p>Robust Scaling.</p></li>
</ul>
<p>This Scaler is robust to outliers, as the name suggests. The mean and standard deviation of the data will not scale well if our data contains several outliers.</p>
<p>For this robust scaling, the median is removed, and the data is scaled according to the interquartile range  The interquartile range (IQR) is the distance between the first and third quartiles (25th and 3rd quantiles) (75th quantile). Because this Scaler’s centering and scaling statistics are based on percentiles, they are unaffected by a few large marginal outliers.</p>
<p>For more details of other scaling features, please follow the links:</p>
<p><a class="reference external" href="https://towardsdatascience.com/all-about-feature-scaling-bcc0ad75cb35">https://towardsdatascience.com/all-about-feature-scaling-bcc0ad75cb35</a></p>
</section>
<section id="feature-scaling-the-fruit-dataset">
<h3><span class="section-number">9.6.2. </span>Feature Scaling the Fruit Dataset<a class="headerlink" href="#feature-scaling-the-fruit-dataset" title="Permalink to this headline">¶</a></h3>
<p>In this section, we will perform min-max scaling to rescaling the features <strong>height</strong> and <strong>color_score</strong> before training the KNN model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#df.head()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">MinMaxScaler</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">RobustScaler</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>


<span class="c1"># Importing a dataset</span>
<span class="n">url</span> <span class="o">=</span> <span class="s1">&#39;https://raw.githubusercontent.com/susanli2016/Machine-Learning-with-Python/master/fruit_data_with_colors.txt&#39;</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_table</span><span class="p">(</span><span class="n">url</span><span class="p">)</span> 

<span class="c1"># Make a copy of the original dataset</span>
<span class="n">df_model</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

<span class="c1">#Rescaling features &#39;mass&#39;,&#39;width&#39;,&#39;height&#39;,&#39;color_score&#39;.</span>
<span class="c1">#scaler = StandardScaler()</span>
<span class="c1">#scaler = RobustScaler()</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">MinMaxScaler</span><span class="p">()</span>


<span class="n">features</span> <span class="o">=</span> <span class="p">[[</span><span class="s1">&#39;mass&#39;</span><span class="p">,</span><span class="s1">&#39;width&#39;</span><span class="p">,</span><span class="s1">&#39;height&#39;</span><span class="p">,</span><span class="s1">&#39;color_score&#39;</span><span class="p">]]</span>
<span class="k">for</span> <span class="n">feature</span> <span class="ow">in</span> <span class="n">features</span><span class="p">:</span>
    <span class="n">df_model</span><span class="p">[</span><span class="n">feature</span><span class="p">]</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">df_model</span><span class="p">[</span><span class="n">feature</span><span class="p">])</span>

<span class="c1">#print(df_model.head())    </span>
    
<span class="c1"># Instantiate the estimator</span>
<span class="n">knn</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">()</span>

<span class="c1">#Create x and y variable</span>
<span class="c1">#X = df_model[[&#39;mass&#39;,&#39;width&#39;,&#39;height&#39;,&#39;color_score&#39;]]</span>
<span class="c1">#y = df_model[&#39;fruit_label&#39;]</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">df_model</span><span class="p">[[</span><span class="s1">&#39;height&#39;</span><span class="p">,</span> <span class="s1">&#39;color_score&#39;</span><span class="p">]]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df_model</span><span class="p">[</span><span class="s1">&#39;fruit_label&#39;</span><span class="p">]</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">k_range</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">11</span><span class="p">)</span>

<span class="n">train_accuracy</span> <span class="o">=</span> <span class="p">{}</span>
<span class="n">train_accuracy_list</span> <span class="o">=</span> <span class="p">[]</span>

<span class="n">test_accuracy</span> <span class="o">=</span> <span class="p">{}</span>
<span class="n">test_accuracy_list</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># misclassfication error</span>
<span class="n">train_error_list</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">test_error_list</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">k_range</span><span class="p">:</span>
    <span class="n">knn</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span> <span class="o">=</span> <span class="n">k</span><span class="p">)</span>
    
    <span class="c1"># Training the classifier by passing in the training set X_train and the labels in y_train</span>
    <span class="n">knn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
    
    <span class="c1"># Compute accuracy on the training set</span>
    <span class="n">train_accuracy</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">knn</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">train_accuracy_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">knn</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">))</span>
    <span class="n">train_error_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">knn</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">))</span>
    
    <span class="n">test_accuracy</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">knn</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span><span class="n">y_test</span><span class="p">)</span>
    <span class="n">test_accuracy_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">knn</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span><span class="n">y_test</span><span class="p">))</span>
    <span class="n">test_error_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">knn</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span><span class="n">y_test</span><span class="p">))</span>
    
<span class="n">df_output</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;k&#39;</span><span class="p">:</span><span class="n">k_range</span><span class="p">,</span>
                          <span class="s1">&#39;train_accuracy&#39;</span><span class="p">:</span><span class="n">train_accuracy_list</span><span class="p">,</span>
                          <span class="s1">&#39;test_accuracy&#39;</span><span class="p">:</span><span class="n">test_accuracy_list</span><span class="p">,</span>
                          <span class="s1">&#39;train_error&#39;</span><span class="p">:</span><span class="n">train_error_list</span><span class="p">,</span>
                          <span class="s1">&#39;test_error&#39;</span><span class="p">:</span><span class="n">test_error_list</span>
                         <span class="p">})</span>   
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df_model</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>fruit_label</th>
      <th>mass</th>
      <th>width</th>
      <th>height</th>
      <th>color_score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>59.000000</td>
      <td>59.000000</td>
      <td>59.000000</td>
      <td>59.000000</td>
      <td>59.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>2.542373</td>
      <td>0.304611</td>
      <td>0.343443</td>
      <td>0.568188</td>
      <td>0.560214</td>
    </tr>
    <tr>
      <th>std</th>
      <td>1.208048</td>
      <td>0.192374</td>
      <td>0.214984</td>
      <td>0.209387</td>
      <td>0.202257</td>
    </tr>
    <tr>
      <th>min</th>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>1.000000</td>
      <td>0.223776</td>
      <td>0.210526</td>
      <td>0.492308</td>
      <td>0.447368</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>3.000000</td>
      <td>0.286713</td>
      <td>0.368421</td>
      <td>0.553846</td>
      <td>0.526316</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>4.000000</td>
      <td>0.353147</td>
      <td>0.447368</td>
      <td>0.646154</td>
      <td>0.684211</td>
    </tr>
    <tr>
      <th>max</th>
      <td>4.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Accuracy over the number of K neighbors</span>

<span class="p">(</span>
    <span class="n">ggplot</span><span class="p">(</span><span class="n">df_output</span><span class="p">)</span> 
    <span class="o">+</span> <span class="n">geom_line</span><span class="p">(</span><span class="n">aes</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="s1">&#39;k&#39;</span><span class="p">,</span>  <span class="n">y</span> <span class="o">=</span> <span class="s1">&#39;train_accuracy&#39;</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;&quot;training accuracy&quot;&#39;</span><span class="p">))</span>
    <span class="o">+</span> <span class="n">geom_line</span><span class="p">(</span><span class="n">aes</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="s1">&#39;k&#39;</span><span class="p">,</span>  <span class="n">y</span> <span class="o">=</span> <span class="s1">&#39;test_accuracy&#39;</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;&quot;testing accuracy&quot;&#39;</span><span class="p">))</span>
    <span class="o">+</span> <span class="n">labs</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s1">&#39;k (n_neighbors)&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s1">&#39;overall accuracy&#39;</span><span class="p">)</span>
    <span class="o">+</span> <span class="n">scale_color_manual</span><span class="p">(</span><span class="n">values</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;blue&quot;</span><span class="p">,</span> <span class="s2">&quot;green&quot;</span><span class="p">],</span> <span class="c1"># Colors</span>
        <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot; &quot;</span><span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/Chapter7_Introduction_Machine_Learning_154_0.png" src="_images/Chapter7_Introduction_Machine_Learning_154_0.png" />
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;ggplot: (309897553)&gt;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Error over the number of K neighbors</span>

<span class="p">(</span>
    <span class="n">ggplot</span><span class="p">(</span><span class="n">df_output</span><span class="p">)</span> 
    <span class="o">+</span> <span class="n">geom_line</span><span class="p">(</span><span class="n">aes</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="s1">&#39;k&#39;</span><span class="p">,</span>  <span class="n">y</span> <span class="o">=</span> <span class="s1">&#39;train_error&#39;</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;&quot;training error&quot;&#39;</span><span class="p">))</span>
    <span class="o">+</span> <span class="n">geom_line</span><span class="p">(</span><span class="n">aes</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="s1">&#39;k&#39;</span><span class="p">,</span>  <span class="n">y</span> <span class="o">=</span> <span class="s1">&#39;test_error&#39;</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;&quot;testing error&quot;&#39;</span><span class="p">))</span>
    <span class="o">+</span> <span class="n">labs</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s1">&#39;Model complexity: k (n_neighbors)&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s1">&#39;Error&#39;</span><span class="p">)</span>
    <span class="o">+</span> <span class="n">scale_color_manual</span><span class="p">(</span><span class="n">values</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;blue&quot;</span><span class="p">,</span> <span class="s2">&quot;green&quot;</span><span class="p">],</span> <span class="c1"># Colors</span>
        <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot; &quot;</span><span class="p">)</span>
    <span class="o">+</span> <span class="n">scale_x_continuous</span><span class="p">(</span><span class="n">trans</span> <span class="o">=</span> <span class="s2">&quot;reverse&quot;</span><span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/Chapter7_Introduction_Machine_Learning_155_0.png" src="_images/Chapter7_Introduction_Machine_Learning_155_0.png" />
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;ggplot: (310429937)&gt;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">row_max</span> <span class="o">=</span> <span class="n">df_output</span><span class="o">.</span><span class="n">test_accuracy</span><span class="o">.</span><span class="n">idxmax</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Best accuracy was </span><span class="si">{</span><span class="n">df_output</span><span class="o">.</span><span class="n">test_accuracy</span><span class="p">[</span><span class="n">row_max</span><span class="p">]</span><span class="si">}</span><span class="s1">, which corresponds to a value of K between 1 and 6&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Best accuracy was 0.8666666666666667, which corresponds to a value of K between 1 and 6
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df_output</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="s1">&#39;test_error&#39;</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>k</th>
      <th>train_accuracy</th>
      <th>test_accuracy</th>
      <th>train_error</th>
      <th>test_error</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>1.000000</td>
      <td>0.866667</td>
      <td>0.000000</td>
      <td>0.133333</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>0.931818</td>
      <td>0.866667</td>
      <td>0.068182</td>
      <td>0.133333</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>0.954545</td>
      <td>0.866667</td>
      <td>0.045455</td>
      <td>0.133333</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>0.931818</td>
      <td>0.866667</td>
      <td>0.068182</td>
      <td>0.133333</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>0.954545</td>
      <td>0.866667</td>
      <td>0.045455</td>
      <td>0.133333</td>
    </tr>
    <tr>
      <th>5</th>
      <td>6</td>
      <td>0.931818</td>
      <td>0.866667</td>
      <td>0.068182</td>
      <td>0.133333</td>
    </tr>
    <tr>
      <th>6</th>
      <td>7</td>
      <td>0.863636</td>
      <td>0.800000</td>
      <td>0.136364</td>
      <td>0.200000</td>
    </tr>
    <tr>
      <th>7</th>
      <td>8</td>
      <td>0.863636</td>
      <td>0.666667</td>
      <td>0.136364</td>
      <td>0.333333</td>
    </tr>
    <tr>
      <th>8</th>
      <td>9</td>
      <td>0.840909</td>
      <td>0.666667</td>
      <td>0.159091</td>
      <td>0.333333</td>
    </tr>
    <tr>
      <th>9</th>
      <td>10</td>
      <td>0.840909</td>
      <td>0.666667</td>
      <td>0.159091</td>
      <td>0.333333</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>From the results above, we see that the performance of KNN model with two features (height and color_score) after rescaling the features increase from 73.33% to values around 86.67% in accuracy (for K between 1 to 6).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">##### Convert pandas DataFrame to Numpy before applying classification</span>

<span class="n">X_train_np</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>
<span class="n">y_train_np</span> <span class="o">=</span> <span class="n">y_train</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>

<span class="n">X_test_np</span> <span class="o">=</span> <span class="n">X_test</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>
<span class="n">y_test_np</span> <span class="o">=</span> <span class="n">y_test</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>

<span class="n">clf</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span> <span class="o">=</span> <span class="mi">6</span><span class="p">)</span>
<span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_np</span><span class="p">,</span> <span class="n">y_train_np</span><span class="p">)</span>

<span class="c1"># Plotting decision regions</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>

<span class="c1"># Decision region for feature 3 = mass = value</span>
<span class="n">value</span><span class="o">=</span><span class="mi">160</span>
<span class="c1"># Plot training sample with feature = mass = value +/- width</span>
<span class="n">width</span><span class="o">=</span><span class="mi">20</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plot_decision_regions</span><span class="p">(</span><span class="n">X_test_np</span><span class="p">,</span> <span class="n">y_test_np</span><span class="p">,</span> <span class="n">clf</span><span class="o">=</span><span class="n">clf</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Feature 1 (height)&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Feature 2 (color_scale)&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Classification problem&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/Users/Kaemyuijang/opt/anaconda3/lib/python3.7/site-packages/mlxtend/plotting/decision_regions.py:279: UserWarning: You passed a edgecolor/edgecolors (&#39;black&#39;) for an unfilled marker (&#39;x&#39;).  Matplotlib is ignoring the edgecolor in favor of the facecolor.  This behavior may change in the future.
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0.5, 1.0, &#39;Classification problem&#39;)
</pre></div>
</div>
<img alt="_images/Chapter7_Introduction_Machine_Learning_159_2.png" src="_images/Chapter7_Introduction_Machine_Learning_159_2.png" />
</div>
</div>
</section>
</section>
<section id="model-validation">
<h2><span class="section-number">9.7. </span>Model validation<a class="headerlink" href="#model-validation" title="Permalink to this headline">¶</a></h2>
<section id="holdout-sets">
<h3><span class="section-number">9.7.1. </span>Holdout sets<a class="headerlink" href="#holdout-sets" title="Permalink to this headline">¶</a></h3>
<p>We can gain a better understanding of a model’s performance by employing a <strong>holdout</strong> (or test) set, which is when we hold back a subset of data from the model’s training and then use it to verify the model’s performance. This may be done <strong>train_test_split</strong> in Scikit-train as previously done.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Importing a dataset</span>
<span class="n">url</span> <span class="o">=</span> <span class="s1">&#39;https://raw.githubusercontent.com/susanli2016/Machine-Learning-with-Python/master/fruit_data_with_colors.txt&#39;</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_table</span><span class="p">(</span><span class="n">url</span><span class="p">)</span> 

<span class="c1"># Make a copy of the original dataset</span>
<span class="n">df_model</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

<span class="c1">#Rescaling features &#39;mass&#39;,&#39;width&#39;,&#39;height&#39;,&#39;color_score&#39;.</span>
<span class="c1">#scaler = StandardScaler()</span>
<span class="c1">#scaler = RobustScaler()</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">MinMaxScaler</span><span class="p">()</span>


<span class="n">features</span> <span class="o">=</span> <span class="p">[[</span><span class="s1">&#39;mass&#39;</span><span class="p">,</span><span class="s1">&#39;width&#39;</span><span class="p">,</span><span class="s1">&#39;height&#39;</span><span class="p">,</span><span class="s1">&#39;color_score&#39;</span><span class="p">]]</span>
<span class="k">for</span> <span class="n">feature</span> <span class="ow">in</span> <span class="n">features</span><span class="p">:</span>
    <span class="n">df_model</span><span class="p">[</span><span class="n">feature</span><span class="p">]</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">df_model</span><span class="p">[</span><span class="n">feature</span><span class="p">])</span>


<span class="c1">#Create x and y variable</span>
<span class="c1">#X = df_model[[&#39;mass&#39;,&#39;width&#39;,&#39;height&#39;,&#39;color_score&#39;]]</span>
<span class="c1">#y = df_model[&#39;fruit_label&#39;]</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">df_model</span><span class="p">[[</span><span class="s1">&#39;height&#39;</span><span class="p">,</span> <span class="s1">&#39;color_score&#39;</span><span class="p">]]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df_model</span><span class="p">[</span><span class="s1">&#39;fruit_label&#39;</span><span class="p">]</span>    
    
<span class="nb">print</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">head</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>     height  color_score
0  0.507692     0.000000
1  0.430769     0.105263
2  0.492308     0.131579
3  0.107692     0.657895
4  0.092308     0.631579
</pre></div>
</div>
</div>
</div>
<p>In this example, we use half of the data as a training set and the other half as the validation set with <code class="docutils literal notranslate"><span class="pre">n_neighbors=5</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X1</span><span class="p">,</span> <span class="n">X2</span><span class="p">,</span> <span class="n">y1</span><span class="p">,</span> <span class="n">y2</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">model1</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

<span class="n">model1</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X1</span><span class="p">,</span><span class="n">y1</span><span class="p">)</span>
<span class="n">accuracy_score</span><span class="p">(</span><span class="n">y2</span><span class="p">,</span> <span class="n">model1</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X2</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.7
</pre></div>
</div>
</div>
</div>
<p>We obtain an accuracy score of 0.7 which indicates that 70% of points were correctly labeled by our model</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># checking the number of fruits by type in the training data</span>

<span class="n">y1</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1    12
4    10
3     5
2     2
Name: fruit_label, dtype: int64
</pre></div>
</div>
</div>
</div>
</section>
<section id="cross-validation">
<h3><span class="section-number">9.7.2. </span>Cross-validation<a class="headerlink" href="#cross-validation" title="Permalink to this headline">¶</a></h3>
<p>One drawback of using a holdout set for model validation is that we lost some of our data during model training. In the preceding example, half of the dataset is not used to train the model! This is inefficient and can lead to issues, particularly if the initial batch of training data is small.</p>
<p>One solution is to employ <strong>cross-validation</strong>, which requires performing a series of fits in which each subset of the data is used as both a training and a validation set.</p>
<p>We perform two validation trials here, utilizing each half of the data as a holdout set alternately.</p>
<p>With the split data from earlier, we could implement it like this:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model1</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">model2</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

<span class="n">model1</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X1</span><span class="p">,</span><span class="n">y1</span><span class="p">)</span>
<span class="n">model2</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X2</span><span class="p">,</span><span class="n">y2</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Accuracy score of model 1:&#39;</span><span class="p">,</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y2</span><span class="p">,</span> <span class="n">model1</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X2</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Accuracy score of model 2:&#39;</span><span class="p">,</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y1</span><span class="p">,</span> <span class="n">model2</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X1</span><span class="p">)))</span>

<span class="c1"># use the average as an estimate of the accuracy</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">An estimate of the accuracy: &#39;</span><span class="p">,(</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y2</span><span class="p">,</span> <span class="n">model1</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X1</span><span class="p">,</span><span class="n">y1</span><span class="p">)</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X2</span><span class="p">))</span> <span class="o">+</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y1</span><span class="p">,</span> <span class="n">model2</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X2</span><span class="p">,</span><span class="n">y2</span><span class="p">)</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X1</span><span class="p">)))</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Accuracy score of model 1: 0.7
Accuracy score of model 2: 0.6896551724137931

An estimate of the accuracy:  0.6948275862068966
</pre></div>
</div>
</div>
</div>
<p>The result is two accuracy scores, which we may combine (for example, by <strong>taking the mean</strong>) to produce a more <strong>accurate estimate</strong> of the global model performance.</p>
<p>This type of cross-validation is a <strong>two-fold cross-validation</strong>, in which the data is split into two sets and each is used as a validation set in turn.</p>
<p>Two-fold cross-validation: image from datavedas
<img alt="Two-fold cross-validation: image from datavedas" src="https://www.datavedas.com/wp-content/uploads/2018/04/image001-1.jpg" /></p>
<p>This notion might be expanded to include more trials and folds in the data—for example, the below figure from <a class="reference external" href="http://scikit-learn.org">scikit-learn.org</a> shows a visual representation of five-fold cross-validation.</p>
<p>Five-fold cross-validation: image from <a class="reference external" href="http://scikit-learn.org">scikit-learn.org</a>
<img alt="Two-fold cross-validation: image from scikit-learn.org" src="https://scikit-learn.org/stable/_images/grid_search_cross_validation.png" /></p>
<p>we can use Scikit-Learn’s cross_val_score convenience routine to do it k-fold cross-validation. The Python code below performs k-fold cross-validation.</p>
<p>In what follows, we perform StratifiedKFold to split the data in 2  folds (defined by <code class="docutils literal notranslate"><span class="pre">n_splits</span> <span class="pre">=</span> <span class="pre">2</span></code>) (see Chapter 5 for more details).</p>
<p>The <code class="docutils literal notranslate"><span class="pre">StratifiedKFold</span></code> module in Scikit-learn sets up <code class="docutils literal notranslate"><span class="pre">n_splits</span></code> (folds, partitions or groups) of the dataset in a way that the folds are made by <strong>preserving the percentage of samples for each class</strong>.</p>
<p>The brief explaination for the code below (also see the diagram below) is as follows:</p>
<ol class="simple">
<li><p>The dataset has been split into K (K = 2 in our example) equal  partitions (or folds).</p></li>
<li><p>(In iteration 1) use fold 1 as the testing set and the union of the other folds as the training set.</p></li>
<li><p>Repeat step 2 for K times, using a different fold as the testing set each time.</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_val_score</span><span class="p">,</span> <span class="n">StratifiedKFold</span>

<span class="n">knn</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

<span class="c1"># Create StratifiedKFold object.</span>
<span class="n">skf</span> <span class="o">=</span> <span class="n">StratifiedKFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># store cross-validation scores in cv_scores object</span>
<span class="n">cv_scores</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">train_index</span><span class="p">,</span> <span class="n">test_index</span> <span class="ow">in</span> <span class="n">skf</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">X_train_fold</span><span class="p">,</span> <span class="n">X_test_fold</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">train_index</span><span class="p">,],</span> <span class="n">X</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">test_index</span><span class="p">,]</span>
    <span class="n">y_train_fold</span><span class="p">,</span> <span class="n">y_test_fold</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">train_index</span><span class="p">,],</span> <span class="n">y</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">test_index</span><span class="p">,]</span>
    <span class="n">knn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_fold</span><span class="p">,</span> <span class="n">y_train_fold</span><span class="p">)</span>
    <span class="n">cv_scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">knn</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test_fold</span><span class="p">,</span> <span class="n">y_test_fold</span><span class="p">))</span>

<span class="nb">print</span><span class="p">(</span><span class="n">cv_scores</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[0.7, 0.7931034482758621]
</pre></div>
</div>
</div>
</div>
<p>The accuracy scores of the two-fold cross-validation are 0.7 and 0.793.</p>
<section id="computing-cross-validated-metrics">
<h4><span class="section-number">9.7.2.1. </span>Computing cross-validated metrics¶<a class="headerlink" href="#computing-cross-validated-metrics" title="Permalink to this headline">¶</a></h4>
<p>We can use Scikit-Learn’s cross_val_score convenience routine to do it k-fold cross-validation without the need to create a StratifiedKFold object (<code class="docutils literal notranslate"><span class="pre">skf</span></code> above). The Python code below computes the cross-validation scores.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_val_score</span>

<span class="n">knn</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

<span class="n">cross_val_score</span><span class="p">(</span><span class="n">knn</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;The accuracy scores after performing two-fold cross-vlaidation are:&#39;</span><span class="p">,</span><span class="n">cross_val_score</span><span class="p">(</span><span class="n">knn</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">2</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The accuracy scores after performing two-fold cross-vlaidation are: [0.66666667 0.5862069 ]
</pre></div>
</div>
</div>
</div>
<p>It is also possible to use other cross validation strategies by passing a cross validation iterator instead, for instance:</p>
<p>See the link below for more details:</p>
<p><a class="reference external" href="https://scikit-learn.org/stable/modules/cross_validation.html">https://scikit-learn.org/stable/modules/cross_validation.html</a></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">ShuffleSplit</span>

<span class="n">knn</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

<span class="n">cv</span> <span class="o">=</span> <span class="n">ShuffleSplit</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">cross_val_score</span><span class="p">(</span><span class="n">knn</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">cv</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([0.8       , 0.86666667])
</pre></div>
</div>
</div>
</div>
<p><strong>Note</strong> The validation scores from the previous results are differents due to the training and test sets are randomly sampled from the original dataset.</p>
<p><strong>Exercise</strong> Plot the (mean) misclassification error over the number of K neighbours to find the optimal K value that gives the maximum mean accuracy score (the mean of the accuracy scores obtained from the two-fold cross-validation).</p>
</section>
</section>
<section id="hypertuning-model-parameters-using-grid-search">
<h3><span class="section-number">9.7.3. </span>Hypertuning Model Parameters using Grid Search<a class="headerlink" href="#hypertuning-model-parameters-using-grid-search" title="Permalink to this headline">¶</a></h3>
<p>In the validation section, we set the parameter ‘n_neighbors’ to 5 as a starting point when we generated our KNN models.</p>
<p>When we go through a process to <strong>identify the best parameters</strong> for our model to improve accuracy, we are <strong>hypertuning parameters</strong>. We will use <strong>GridSearchCV</strong> to find the best value for ‘n_neighbors’ in our situation.</p>
<p><strong>GridSearchCV</strong> works by repeatedly training our model on a set of parameters that we define. That manner, we can test our model with each parameter and determine the best settings for maximum accuracy.
In order to see which value for ‘n_neighbors’ works best for our model, we will specify a range of values.</p>
<p>To perform so, we’ll make a dictionary with the key ‘n_neighbors’ and use numpy to build an array of values ranging from 1 to 15.
In order to determine the best value for ‘n_neighbors,’ our new grid search model will use a new k-NN classifier, our param grid, and a cross-validation value of 2 (use 2 in our case why?).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Importing a dataset</span>
<span class="n">url</span> <span class="o">=</span> <span class="s1">&#39;https://raw.githubusercontent.com/susanli2016/Machine-Learning-with-Python/master/fruit_data_with_colors.txt&#39;</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_table</span><span class="p">(</span><span class="n">url</span><span class="p">)</span> 

<span class="c1"># Make a copy of the original dataset</span>
<span class="n">df_model</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

<span class="c1">#Rescaling features &#39;mass&#39;,&#39;width&#39;,&#39;height&#39;,&#39;color_score&#39;.</span>
<span class="c1">#scaler = StandardScaler()</span>
<span class="c1">#scaler = RobustScaler()</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">MinMaxScaler</span><span class="p">()</span>


<span class="n">features</span> <span class="o">=</span> <span class="p">[[</span><span class="s1">&#39;mass&#39;</span><span class="p">,</span><span class="s1">&#39;width&#39;</span><span class="p">,</span><span class="s1">&#39;height&#39;</span><span class="p">,</span><span class="s1">&#39;color_score&#39;</span><span class="p">]]</span>
<span class="k">for</span> <span class="n">feature</span> <span class="ow">in</span> <span class="n">features</span><span class="p">:</span>
    <span class="n">df_model</span><span class="p">[</span><span class="n">feature</span><span class="p">]</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">df_model</span><span class="p">[</span><span class="n">feature</span><span class="p">])</span>


<span class="c1">#Create X and y variable</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">df_model</span><span class="p">[[</span><span class="s1">&#39;height&#39;</span><span class="p">,</span> <span class="s1">&#39;color_score&#39;</span><span class="p">]]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df_model</span><span class="p">[</span><span class="s1">&#39;fruit_label&#39;</span><span class="p">]</span>    
    
<span class="c1"># Split the dataset into training and testing sets    </span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>To get started, we need to import <code class="docutils literal notranslate"><span class="pre">GridSearchCV</span></code> from the Sklearn library.</p>
<p>The model we use for hyperparameter tuning is required (<code class="docutils literal notranslate"><span class="pre">knn</span></code> in our case) by the <strong>estimator parameter</strong> of GridSearchCV.</p>
<p>A list of parameters and the range of values for each parameter of the specified estimator are required by the <strong>param_grid parameter</strong>. When working with the KNN model, the most important parameter is <strong>n_neighbors</strong>.</p>
<p>Cross-validation is performed to determine the hyperparameter value set that provides the best accuracy.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">GridSearchCV</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># the estimator used in out example</span>
<span class="n">knn</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">()</span>

<span class="c1"># create a parameter grid: map the parameter names to the values that should be searched</span>
<span class="c1"># simply a python dictionary</span>
<span class="c1"># key: parameter name</span>
<span class="c1"># value: list of values that should be searched for that parameter</span>
<span class="c1"># single key-value pair for param_grid</span>
<span class="n">param_grid</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;n_neighbors&#39;</span><span class="p">:</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">16</span><span class="p">)}</span>

<span class="c1">#k_range = list(range(1, 16))</span>
<span class="c1">#param_grid = dict(n_neighbors=k_range)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># instantiate the grid</span>
<span class="n">grid</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">knn</span><span class="p">,</span> <span class="n">param_grid</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;accuracy&#39;</span><span class="p">)</span>

<span class="c1">#grid = GridSearchCV(knn, param_grid, cv=2, scoring=&#39;balanced_accuracy&#39;)</span>
</pre></div>
</div>
</div>
</div>
<p>The grid object is ready for 2-fold cross-validation of a KNN model with classification accuracy as the evaluation measure.</p>
<ul class="simple">
<li><p>There is also a grid parameter to repeat the 2-fold cross-validation 15 times.</p></li>
<li><p>Each time, the parameter <strong>n_neighbors</strong> should get a different value from the list.</p></li>
<li><p>Here we specify that n_neighbors should take the values 1 to 15.</p></li>
<li><p>We can designate a scorer object with the scoring parameter, <code class="docutils literal notranslate"><span class="pre">scoring='accuracy'</span></code> in this example. (for more details <a class="reference external" href="https://scikit-learn.org/stable/modules/model_evaluation.html">https://scikit-learn.org/stable/modules/model_evaluation.html</a>)</p></li>
<li><p>You can set n_jobs = -1 to run calculations in parallel (if your computer and OS support it). This is also called <strong>parallel programming</strong>.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># fit the grid with the training set</span>
<span class="n">grid</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>GridSearchCV(cv=2, estimator=KNeighborsClassifier(),
             param_grid={&#39;n_neighbors&#39;: array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15])},
             scoring=&#39;accuracy&#39;)
</pre></div>
</div>
</div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">cv_results_</span></code> returns a dictionary of all the evaluation metrics from the gridsearch. To visualize it properly, you can convert into a pandas DataFrame. The results include the test scores for the two folds, the mean and standard deviation of the test scores and also the ranking of the mean test scores.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># view the complete results </span>
<span class="c1"># cv_results_dict: this attributes gives a dict with keys as column headers </span>
<span class="c1"># and values as columns, that can be imported into a pandas DataFrame. </span>

<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">grid</span><span class="o">.</span><span class="n">cv_results_</span><span class="p">)</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>mean_fit_time</th>
      <th>std_fit_time</th>
      <th>mean_score_time</th>
      <th>std_score_time</th>
      <th>param_n_neighbors</th>
      <th>params</th>
      <th>split0_test_score</th>
      <th>split1_test_score</th>
      <th>mean_test_score</th>
      <th>std_test_score</th>
      <th>rank_test_score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.005679</td>
      <td>0.002084</td>
      <td>0.005561</td>
      <td>0.000824</td>
      <td>1</td>
      <td>{'n_neighbors': 1}</td>
      <td>0.636364</td>
      <td>0.909091</td>
      <td>0.772727</td>
      <td>0.136364</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.002670</td>
      <td>0.000004</td>
      <td>0.006662</td>
      <td>0.003166</td>
      <td>2</td>
      <td>{'n_neighbors': 2}</td>
      <td>0.727273</td>
      <td>0.772727</td>
      <td>0.750000</td>
      <td>0.022727</td>
      <td>2</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.002560</td>
      <td>0.000067</td>
      <td>0.003976</td>
      <td>0.000336</td>
      <td>3</td>
      <td>{'n_neighbors': 3}</td>
      <td>0.636364</td>
      <td>0.772727</td>
      <td>0.704545</td>
      <td>0.068182</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.007777</td>
      <td>0.005566</td>
      <td>0.005177</td>
      <td>0.001293</td>
      <td>4</td>
      <td>{'n_neighbors': 4}</td>
      <td>0.500000</td>
      <td>0.772727</td>
      <td>0.636364</td>
      <td>0.136364</td>
      <td>4</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.002213</td>
      <td>0.000024</td>
      <td>0.003185</td>
      <td>0.000199</td>
      <td>5</td>
      <td>{'n_neighbors': 5}</td>
      <td>0.545455</td>
      <td>0.681818</td>
      <td>0.613636</td>
      <td>0.068182</td>
      <td>5</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create a list of the mean scores </span>

<span class="n">grid_mean_scores</span> <span class="o">=</span> <span class="n">grid</span><span class="o">.</span><span class="n">cv_results_</span><span class="p">[</span><span class="s1">&#39;mean_test_score&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>Plot the value of K for KNN (x-axis) against the cross-validated accuracy (y-axis).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plot the result</span>
<span class="n">k_range</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">16</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">k_range</span><span class="p">,</span> <span class="n">grid_mean_scores</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Value of K for KNN&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Cross-Validated Accuracy&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0, 0.5, &#39;Cross-Validated Accuracy&#39;)
</pre></div>
</div>
<img alt="_images/Chapter7_Introduction_Machine_Learning_196_1.png" src="_images/Chapter7_Introduction_Machine_Learning_196_1.png" />
</div>
</div>
<p>After training, we can check which of the values we tested for ‘n_neighbors’ worked best. For this purpose we call ‘best_params_’ in our model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># examine the best model</span>

<span class="c1"># Single best score achieved across all params (k)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">grid</span><span class="o">.</span><span class="n">best_score_</span><span class="p">)</span>

<span class="c1"># Dictionary containing the parameters (k) used to generate that score</span>
<span class="nb">print</span><span class="p">(</span><span class="n">grid</span><span class="o">.</span><span class="n">best_params_</span><span class="p">)</span>

<span class="c1"># Actual model object fit with those best parameters</span>
<span class="c1"># Shows default parameters that we did not specify</span>
<span class="nb">print</span><span class="p">(</span><span class="n">grid</span><span class="o">.</span><span class="n">best_estimator_</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.7727272727272727
{&#39;n_neighbors&#39;: 1}
KNeighborsClassifier(n_neighbors=1)
</pre></div>
</div>
</div>
</div>
<p>The results show that the cross-validated accuracy is 77.27% when the value of the K is 1.</p>
<section id="using-the-best-parameter-to-make-prediction-and-result-evaluation">
<h4><span class="section-number">9.7.3.1. </span>Using the best parameter to make prediction and result evaluation<a class="headerlink" href="#using-the-best-parameter-to-make-prediction-and-result-evaluation" title="Permalink to this headline">¶</a></h4>
<p>The classfification report on the validation set can be created on a per-class basis. It provides a deeper understanding of the behavior of the classifier than global accuracy, which can disguise functional weaknesses in a class of a multiclass problem.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Using the best parameter obtained earlier</span>

<span class="n">best_model</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Train out best_model on the training set</span>
<span class="n">best_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Make predictions on the test set</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">best_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We can crate the confusion metrix and the summary of the predictions.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Confusion Matrix visualization.</span>

<span class="n">cm</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">best_model</span><span class="o">.</span><span class="n">classes_</span><span class="p">)</span>
<span class="n">disp</span> <span class="o">=</span> <span class="n">ConfusionMatrixDisplay</span><span class="p">(</span><span class="n">confusion_matrix</span><span class="o">=</span><span class="n">cm</span><span class="p">,</span><span class="n">display_labels</span><span class="o">=</span><span class="n">best_model</span><span class="o">.</span><span class="n">classes_</span><span class="p">)</span>
<span class="n">disp</span><span class="o">.</span><span class="n">plot</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x1287f8e10&gt;
</pre></div>
</div>
<img alt="_images/Chapter7_Introduction_Machine_Learning_203_1.png" src="_images/Chapter7_Introduction_Machine_Learning_203_1.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Classification report: </span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Classification report: 

              precision    recall  f1-score   support

           1       1.00      1.00      1.00         4
           2       1.00      1.00      1.00         1
           3       1.00      0.75      0.86         8
           4       0.50      1.00      0.67         2

    accuracy                           0.87        15
   macro avg       0.88      0.94      0.88        15
weighted avg       0.93      0.87      0.88        15
</pre></div>
</div>
</div>
</div>
<p>Observation of the classification report for the prediction model as follows.</p>
<ul class="simple">
<li><p>The classifier made a total of 15 predictions.</p></li>
<li><p>Out of those 15 samples, the classifier predicted 4 apples, 1 mandarins, 6 oranges and 4 lemons.</p></li>
<li><p>In reality, out of 4 predicted lemons, two of them are oranges.</p></li>
</ul>
<p>The overall accuracy of the prediction of the fruit data set by applying the KNN classifier model is 86.67 which means the model performs well in our example.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">y_pred</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.8666666666666667
</pre></div>
</div>
</div>
</div>
</section>
<section id="swarmplots">
<h4><span class="section-number">9.7.3.2. </span>Swarmplots<a class="headerlink" href="#swarmplots" title="Permalink to this headline">¶</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df_model</span><span class="o">.</span><span class="n">columns</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Index([&#39;fruit_label&#39;, &#39;fruit_name&#39;, &#39;fruit_subtype&#39;, &#39;mass&#39;, &#39;width&#39;, &#39;height&#39;,
       &#39;color_score&#39;],
      dtype=&#39;object&#39;)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#df_model[[&#39;fruit_name&#39;,&#39;mass&#39;, &#39;width&#39;,&#39;height&#39;, &#39;color_score&#39;]]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">melt</span><span class="p">(</span><span class="n">df_model</span><span class="p">[[</span><span class="s1">&#39;fruit_name&#39;</span><span class="p">,</span><span class="s1">&#39;mass&#39;</span><span class="p">,</span> <span class="s1">&#39;width&#39;</span><span class="p">,</span><span class="s1">&#39;height&#39;</span><span class="p">,</span> <span class="s1">&#39;color_score&#39;</span><span class="p">]],</span> <span class="n">id_vars</span> <span class="o">=</span> <span class="s1">&#39;fruit_name&#39;</span><span class="p">,</span> <span class="n">var_name</span> <span class="o">=</span> <span class="s1">&#39;features&#39;</span><span class="p">,</span><span class="n">value_name</span> <span class="o">=</span> <span class="s1">&#39;value&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#data</span>
<span class="c1"># swarmplot for analysing the different attributes</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
<span class="n">sns</span><span class="o">.</span><span class="n">swarmplot</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="s1">&#39;features&#39;</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="s1">&#39;value&#39;</span><span class="p">,</span> <span class="n">hue</span> <span class="o">=</span> <span class="s1">&#39;fruit_name&#39;</span><span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/Chapter7_Introduction_Machine_Learning_218_0.png" src="_images/Chapter7_Introduction_Machine_Learning_218_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">df_model</span><span class="p">[[</span><span class="s1">&#39;mass&#39;</span><span class="p">,</span> <span class="s1">&#39;width&#39;</span><span class="p">,</span><span class="s1">&#39;height&#39;</span><span class="p">,</span> <span class="s1">&#39;color_score&#39;</span><span class="p">]]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df_model</span><span class="p">[</span><span class="s1">&#39;fruit_label&#39;</span><span class="p">]</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">chi2</span><span class="p">,</span> <span class="n">SelectKBest</span><span class="p">,</span> <span class="n">f_classif</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Get the two best(k = 2) features using the SelectKBest method</span>
<span class="n">ft</span> <span class="o">=</span> <span class="n">SelectKBest</span><span class="p">(</span><span class="n">chi2</span><span class="p">,</span> <span class="n">k</span> <span class="o">=</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Score: &#39;</span><span class="p">,</span> <span class="n">ft</span><span class="o">.</span><span class="n">scores_</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Columns: &#39;</span><span class="p">,</span> <span class="n">X_train</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Score:  [2.0299807  3.68664781 2.56587049 0.43604189]
Columns:  Index([&#39;mass&#39;, &#39;width&#39;, &#39;height&#39;, &#39;color_score&#39;], dtype=&#39;object&#39;)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ft</span> <span class="o">=</span> <span class="n">SelectKBest</span><span class="p">(</span><span class="n">f_classif</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Score: &#39;</span><span class="p">,</span> <span class="n">ft</span><span class="o">.</span><span class="n">scores_</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Columns: &#39;</span><span class="p">,</span> <span class="n">X_train</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Score:  [ 8.69254183 18.22929591 40.53528918  2.26802478]
Columns:  Index([&#39;mass&#39;, &#39;width&#39;, &#39;height&#39;, &#39;color_score&#39;], dtype=&#39;object&#39;)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Importing a dataset</span>
<span class="n">url</span> <span class="o">=</span> <span class="s1">&#39;https://raw.githubusercontent.com/susanli2016/Machine-Learning-with-Python/master/fruit_data_with_colors.txt&#39;</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_table</span><span class="p">(</span><span class="n">url</span><span class="p">)</span> 

<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">MinMaxScaler</span>

<span class="n">features</span> <span class="o">=</span> <span class="p">[[</span><span class="s1">&#39;mass&#39;</span><span class="p">,</span><span class="s1">&#39;width&#39;</span><span class="p">,</span><span class="s1">&#39;height&#39;</span><span class="p">,</span><span class="s1">&#39;color_score&#39;</span><span class="p">]]</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="p">[[</span><span class="s1">&#39;mass&#39;</span><span class="p">,</span><span class="s1">&#39;width&#39;</span><span class="p">,</span><span class="s1">&#39;height&#39;</span><span class="p">,</span><span class="s1">&#39;color_score&#39;</span><span class="p">]]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;fruit_label&#39;</span><span class="p">]</span>

<span class="n">scaler</span> <span class="o">=</span> <span class="n">MinMaxScaler</span><span class="p">()</span>

<span class="n">scaled_X</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">),</span>
                        <span class="n">columns</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">scaled_X</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>mass</th>
      <th>width</th>
      <th>height</th>
      <th>color_score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.405594</td>
      <td>0.684211</td>
      <td>0.507692</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.363636</td>
      <td>0.578947</td>
      <td>0.430769</td>
      <td>0.105263</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.349650</td>
      <td>0.421053</td>
      <td>0.492308</td>
      <td>0.131579</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.034965</td>
      <td>0.105263</td>
      <td>0.107692</td>
      <td>0.657895</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.027972</td>
      <td>0.052632</td>
      <td>0.092308</td>
      <td>0.631579</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">scaled_df</span> <span class="o">=</span> <span class="n">scaled_X</span>
<span class="n">scaled_df</span><span class="p">[</span><span class="s1">&#39;fruit_label&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[[</span><span class="s1">&#39;fruit_label&#39;</span><span class="p">]]</span>
<span class="c1">#scaled_df</span>

<span class="n">hm</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">scaled_df</span><span class="o">.</span><span class="n">corr</span><span class="p">(),</span> <span class="n">annot</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
<span class="n">hm</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">title</span> <span class="o">=</span> <span class="s2">&quot;Correlation matrix of insurance data</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/Chapter7_Introduction_Machine_Learning_229_0.png" src="_images/Chapter7_Introduction_Machine_Learning_229_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sns</span><span class="o">.</span><span class="n">pairplot</span><span class="p">(</span><span class="n">scaled_df</span><span class="p">[[</span><span class="s1">&#39;fruit_label&#39;</span><span class="p">,</span><span class="s1">&#39;color_score&#39;</span><span class="p">,</span><span class="s1">&#39;height&#39;</span><span class="p">]],</span><span class="n">hue</span><span class="o">=</span><span class="s1">&#39;fruit_label&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;seaborn.axisgrid.PairGrid at 0x127982c50&gt;
</pre></div>
</div>
<img alt="_images/Chapter7_Introduction_Machine_Learning_230_1.png" src="_images/Chapter7_Introduction_Machine_Learning_230_1.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Importing a dataset</span>
<span class="c1">#url = &#39;https://raw.githubusercontent.com/susanli2016/Machine-Learning-with-Python/master/fruit_data_with_colors.txt&#39;</span>
<span class="c1">#df = pd.read_table(url)  </span>

<span class="c1"># Train Test Split</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">scaled_df</span><span class="p">[[</span><span class="s1">&#39;height&#39;</span><span class="p">,</span> <span class="s1">&#39;color_score&#39;</span><span class="p">]]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">scaled_df</span><span class="p">[</span><span class="s1">&#39;fruit_label&#39;</span><span class="p">]</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Instantiate the estimator</span>
<span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>
<span class="n">knn</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">()</span>

<span class="n">k_range</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">11</span><span class="p">)</span>

<span class="n">train_accuracy</span> <span class="o">=</span> <span class="p">{}</span>
<span class="n">train_accuracy_list</span> <span class="o">=</span> <span class="p">[]</span>

<span class="n">test_accuracy</span> <span class="o">=</span> <span class="p">{}</span>
<span class="n">test_accuracy_list</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">k_range</span><span class="p">:</span>
    <span class="n">knn</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span> <span class="o">=</span> <span class="n">k</span><span class="p">)</span>
    
    <span class="c1"># Training the classifier by passing in the training set X_train and the labels in y_train</span>
    <span class="n">knn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
    
    <span class="c1"># Compute accuracy on the training set</span>
    <span class="n">train_accuracy</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">knn</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">train_accuracy_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">knn</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">))</span>
    
    
    <span class="n">test_accuracy</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">knn</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span><span class="n">y_test</span><span class="p">)</span>
    <span class="n">test_accuracy_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">knn</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span><span class="n">y_test</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">train_accuracy</span>
<span class="nb">print</span><span class="p">(</span><span class="n">test_accuracy</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{1: 0.8666666666666667, 2: 0.8666666666666667, 3: 0.8666666666666667, 4: 0.8666666666666667, 5: 0.8666666666666667, 6: 0.8666666666666667, 7: 0.8, 8: 0.6666666666666666, 9: 0.6666666666666666, 10: 0.6666666666666666}
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#scaled_df_output</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">scaled_df_output</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;k&#39;</span><span class="p">:</span><span class="n">k_range</span><span class="p">,</span>
                          <span class="s1">&#39;train_accuracy&#39;</span><span class="p">:</span><span class="n">train_accuracy_list</span><span class="p">,</span>
                          <span class="s1">&#39;test_accuracy&#39;</span><span class="p">:</span><span class="n">test_accuracy_list</span>
                         <span class="p">})</span>

<span class="p">(</span>
    <span class="n">ggplot</span><span class="p">(</span><span class="n">scaled_df_output</span><span class="p">)</span> 
    <span class="o">+</span> <span class="n">geom_line</span><span class="p">(</span><span class="n">aes</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="s1">&#39;k&#39;</span><span class="p">,</span>  <span class="n">y</span> <span class="o">=</span> <span class="s1">&#39;train_accuracy&#39;</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;&quot;training accuracy&quot;&#39;</span><span class="p">))</span>
    <span class="o">+</span> <span class="n">geom_line</span><span class="p">(</span><span class="n">aes</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="s1">&#39;k&#39;</span><span class="p">,</span>  <span class="n">y</span> <span class="o">=</span> <span class="s1">&#39;test_accuracy&#39;</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;&quot;testing accuracy&quot;&#39;</span><span class="p">))</span>
    <span class="o">+</span> <span class="n">labs</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s1">&#39;k (n_neighbors)&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s1">&#39;overall accuracy&#39;</span><span class="p">)</span>
    <span class="o">+</span> <span class="n">scale_color_manual</span><span class="p">(</span><span class="n">values</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;blue&quot;</span><span class="p">,</span> <span class="s2">&quot;green&quot;</span><span class="p">],</span> <span class="c1"># Colors</span>
        <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot; &quot;</span><span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/Chapter7_Introduction_Machine_Learning_235_0.png" src="_images/Chapter7_Introduction_Machine_Learning_235_0.png" />
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;ggplot: (300097561)&gt;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">##### Convert pandas DataFrame to Numpy before applying classification</span>

<span class="n">X_train_np</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>
<span class="n">y_train_np</span> <span class="o">=</span> <span class="n">y_train</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>

<span class="n">X_test_np</span> <span class="o">=</span> <span class="n">X_test</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>
<span class="n">y_test_np</span> <span class="o">=</span> <span class="n">y_test</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>

<span class="n">clf</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span> <span class="o">=</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_np</span><span class="p">,</span> <span class="n">y_train_np</span><span class="p">)</span>

<span class="c1"># Plotting decision regions</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>

<span class="c1"># Decision region for feature 3 = mass = value</span>
<span class="n">value</span><span class="o">=</span><span class="mi">160</span>
<span class="c1"># Plot training sample with feature = mass = value +/- width</span>
<span class="n">width</span><span class="o">=</span><span class="mi">20</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plot_decision_regions</span><span class="p">(</span><span class="n">X_test_np</span><span class="p">,</span> <span class="n">y_test_np</span><span class="p">,</span> <span class="n">clf</span><span class="o">=</span><span class="n">clf</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Feature 1 (height)&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Feature 2 (color_scale)&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Classification problem&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/Users/Kaemyuijang/opt/anaconda3/lib/python3.7/site-packages/mlxtend/plotting/decision_regions.py:279: UserWarning: You passed a edgecolor/edgecolors (&#39;black&#39;) for an unfilled marker (&#39;x&#39;).  Matplotlib is ignoring the edgecolor in favor of the facecolor.  This behavior may change in the future.
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0.5, 1.0, &#39;Classification problem&#39;)
</pre></div>
</div>
<img alt="_images/Chapter7_Introduction_Machine_Learning_236_2.png" src="_images/Chapter7_Introduction_Machine_Learning_236_2.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">test_accuracy</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{1: 0.8666666666666667,
 2: 0.8666666666666667,
 3: 0.8666666666666667,
 4: 0.8666666666666667,
 5: 0.8666666666666667,
 6: 0.8666666666666667,
 7: 0.8,
 8: 0.6666666666666666,
 9: 0.6666666666666666,
 10: 0.6666666666666666}
</pre></div>
</div>
</div>
</div>
</section>
<section id="feature-scaling-the-fruit-dataset-width-and-height">
<h4><span class="section-number">9.7.3.3. </span>Feature Scaling the Fruit Dataset (width and height)<a class="headerlink" href="#feature-scaling-the-fruit-dataset-width-and-height" title="Permalink to this headline">¶</a></h4>
<p>In this section, we will perform min-max scaling to rescaling the features <strong>height</strong> and <strong>color_score</strong> before training the KNN model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#df.head()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">MinMaxScaler</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">RobustScaler</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>


<span class="c1"># Importing a dataset</span>
<span class="n">url</span> <span class="o">=</span> <span class="s1">&#39;https://raw.githubusercontent.com/susanli2016/Machine-Learning-with-Python/master/fruit_data_with_colors.txt&#39;</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_table</span><span class="p">(</span><span class="n">url</span><span class="p">)</span> 

<span class="c1"># Make a copy of the original dataset</span>
<span class="n">df_model</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

<span class="c1">#Rescaling features &#39;mass&#39;,&#39;width&#39;,&#39;height&#39;,&#39;color_score&#39;.</span>
<span class="c1">#scaler = StandardScaler()</span>
<span class="c1">#scaler = RobustScaler()</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">MinMaxScaler</span><span class="p">()</span>


<span class="n">features</span> <span class="o">=</span> <span class="p">[[</span><span class="s1">&#39;mass&#39;</span><span class="p">,</span><span class="s1">&#39;width&#39;</span><span class="p">,</span><span class="s1">&#39;height&#39;</span><span class="p">,</span><span class="s1">&#39;color_score&#39;</span><span class="p">]]</span>
<span class="k">for</span> <span class="n">feature</span> <span class="ow">in</span> <span class="n">features</span><span class="p">:</span>
    <span class="n">df_model</span><span class="p">[</span><span class="n">feature</span><span class="p">]</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">df_model</span><span class="p">[</span><span class="n">feature</span><span class="p">])</span>

<span class="c1">#print(df_model.head())    </span>
    
<span class="c1"># Instantiate the estimator</span>
<span class="n">knn</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">()</span>

<span class="c1">#Create x and y variable</span>
<span class="c1">#X = df_model[[&#39;mass&#39;,&#39;width&#39;,&#39;height&#39;,&#39;color_score&#39;]]</span>
<span class="c1">#y = df_model[&#39;fruit_label&#39;]</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">df_model</span><span class="p">[[</span><span class="s1">&#39;height&#39;</span><span class="p">,</span> <span class="s1">&#39;width&#39;</span><span class="p">]]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df_model</span><span class="p">[</span><span class="s1">&#39;fruit_label&#39;</span><span class="p">]</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">k_range</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">11</span><span class="p">)</span>

<span class="n">train_accuracy</span> <span class="o">=</span> <span class="p">{}</span>
<span class="n">train_accuracy_list</span> <span class="o">=</span> <span class="p">[]</span>

<span class="n">test_accuracy</span> <span class="o">=</span> <span class="p">{}</span>
<span class="n">test_accuracy_list</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># misclassfication error</span>
<span class="n">train_error_list</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">test_error_list</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">k_range</span><span class="p">:</span>
    <span class="n">knn</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span> <span class="o">=</span> <span class="n">k</span><span class="p">)</span>
    
    <span class="c1"># Training the classifier by passing in the training set X_train and the labels in y_train</span>
    <span class="n">knn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
    
    <span class="c1"># Compute accuracy on the training set</span>
    <span class="n">train_accuracy</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">knn</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">train_accuracy_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">knn</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">))</span>
    <span class="n">train_error_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">knn</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">))</span>
    
    <span class="n">test_accuracy</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">knn</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span><span class="n">y_test</span><span class="p">)</span>
    <span class="n">test_accuracy_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">knn</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span><span class="n">y_test</span><span class="p">))</span>
    <span class="n">test_error_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">knn</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span><span class="n">y_test</span><span class="p">))</span>
    
<span class="n">df_output</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;k&#39;</span><span class="p">:</span><span class="n">k_range</span><span class="p">,</span>
                          <span class="s1">&#39;train_accuracy&#39;</span><span class="p">:</span><span class="n">train_accuracy_list</span><span class="p">,</span>
                          <span class="s1">&#39;test_accuracy&#39;</span><span class="p">:</span><span class="n">test_accuracy_list</span><span class="p">,</span>
                          <span class="s1">&#39;train_error&#39;</span><span class="p">:</span><span class="n">train_error_list</span><span class="p">,</span>
                          <span class="s1">&#39;test_error&#39;</span><span class="p">:</span><span class="n">test_error_list</span>
                         <span class="p">})</span>   
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Accuracy over the number of K neighbors</span>

<span class="p">(</span>
    <span class="n">ggplot</span><span class="p">(</span><span class="n">df_output</span><span class="p">)</span> 
    <span class="o">+</span> <span class="n">geom_line</span><span class="p">(</span><span class="n">aes</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="s1">&#39;k&#39;</span><span class="p">,</span>  <span class="n">y</span> <span class="o">=</span> <span class="s1">&#39;train_accuracy&#39;</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;&quot;training accuracy&quot;&#39;</span><span class="p">))</span>
    <span class="o">+</span> <span class="n">geom_line</span><span class="p">(</span><span class="n">aes</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="s1">&#39;k&#39;</span><span class="p">,</span>  <span class="n">y</span> <span class="o">=</span> <span class="s1">&#39;test_accuracy&#39;</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;&quot;testing accuracy&quot;&#39;</span><span class="p">))</span>
    <span class="o">+</span> <span class="n">labs</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s1">&#39;k (n_neighbors)&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s1">&#39;overall accuracy&#39;</span><span class="p">)</span>
    <span class="o">+</span> <span class="n">scale_color_manual</span><span class="p">(</span><span class="n">values</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;blue&quot;</span><span class="p">,</span> <span class="s2">&quot;green&quot;</span><span class="p">],</span> <span class="c1"># Colors</span>
        <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot; &quot;</span><span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/Chapter7_Introduction_Machine_Learning_248_0.png" src="_images/Chapter7_Introduction_Machine_Learning_248_0.png" />
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;ggplot: (313184393)&gt;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Error over the number of K neighbors</span>

<span class="p">(</span>
    <span class="n">ggplot</span><span class="p">(</span><span class="n">df_output</span><span class="p">)</span> 
    <span class="o">+</span> <span class="n">geom_line</span><span class="p">(</span><span class="n">aes</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="s1">&#39;k&#39;</span><span class="p">,</span>  <span class="n">y</span> <span class="o">=</span> <span class="s1">&#39;train_error&#39;</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;&quot;training error&quot;&#39;</span><span class="p">))</span>
    <span class="o">+</span> <span class="n">geom_line</span><span class="p">(</span><span class="n">aes</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="s1">&#39;k&#39;</span><span class="p">,</span>  <span class="n">y</span> <span class="o">=</span> <span class="s1">&#39;test_error&#39;</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;&quot;testing error&quot;&#39;</span><span class="p">))</span>
    <span class="o">+</span> <span class="n">labs</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s1">&#39;Model complexity: k (n_neighbors)&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s1">&#39;Error&#39;</span><span class="p">)</span>
    <span class="o">+</span> <span class="n">scale_color_manual</span><span class="p">(</span><span class="n">values</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;blue&quot;</span><span class="p">,</span> <span class="s2">&quot;green&quot;</span><span class="p">],</span> <span class="c1"># Colors</span>
        <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot; &quot;</span><span class="p">)</span>
    <span class="o">+</span> <span class="n">scale_x_continuous</span><span class="p">(</span><span class="n">trans</span> <span class="o">=</span> <span class="s2">&quot;reverse&quot;</span><span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/Chapter7_Introduction_Machine_Learning_249_0.png" src="_images/Chapter7_Introduction_Machine_Learning_249_0.png" />
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;ggplot: (312075721)&gt;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">row_max</span> <span class="o">=</span> <span class="n">df_output</span><span class="o">.</span><span class="n">test_accuracy</span><span class="o">.</span><span class="n">idxmax</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Best accuracy was </span><span class="si">{</span><span class="n">df_output</span><span class="o">.</span><span class="n">test_accuracy</span><span class="p">[</span><span class="n">row_max</span><span class="p">]</span><span class="si">}</span><span class="s1">, which corresponds to a value of K between 1 and 6&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Best accuracy was 0.6666666666666666, which corresponds to a value of K between 1 and 6
</pre></div>
</div>
</div>
</div>
<p>From the results above, we see that the performance of KNN model with two features (height and color_score) increase from 73.33% to values around 86.67% in accuracy.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">##### Convert pandas DataFrame to Numpy before applying classification</span>

<span class="n">X_train_np</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>
<span class="n">y_train_np</span> <span class="o">=</span> <span class="n">y_train</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>

<span class="n">X_test_np</span> <span class="o">=</span> <span class="n">X_test</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>
<span class="n">y_test_np</span> <span class="o">=</span> <span class="n">y_test</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>

<span class="n">clf</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span> <span class="o">=</span> <span class="mi">6</span><span class="p">)</span>
<span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_np</span><span class="p">,</span> <span class="n">y_train_np</span><span class="p">)</span>

<span class="c1"># Plotting decision regions</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>

<span class="c1"># Decision region for feature 3 = mass = value</span>
<span class="c1">#value=160</span>
<span class="c1"># Plot training sample with feature = mass = value +/- width</span>
<span class="c1">#width=20</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plot_decision_regions</span><span class="p">(</span><span class="n">X_test_np</span><span class="p">,</span> <span class="n">y_test_np</span><span class="p">,</span> <span class="n">clf</span><span class="o">=</span><span class="n">clf</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Feature 1 (height)&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Feature 2 (width)&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Classification problem&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/Users/Kaemyuijang/opt/anaconda3/lib/python3.7/site-packages/mlxtend/plotting/decision_regions.py:279: UserWarning: You passed a edgecolor/edgecolors (&#39;black&#39;) for an unfilled marker (&#39;x&#39;).  Matplotlib is ignoring the edgecolor in favor of the facecolor.  This behavior may change in the future.
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0.5, 1.0, &#39;Classification problem&#39;)
</pre></div>
</div>
<img alt="_images/Chapter7_Introduction_Machine_Learning_252_2.png" src="_images/Chapter7_Introduction_Machine_Learning_252_2.png" />
</div>
</div>
</section>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="Chapter6_Regression.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title"><span class="section-number">8. </span>Regression Analysis</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="Chapter8_Unsupervised_Learning.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">10. </span>Unsupervised Machine Learning: K-means Clustering</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By Pairote Satiracoo<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>